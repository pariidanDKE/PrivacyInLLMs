{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore ULD Method \n",
    "\n",
    "1. Dataset\n",
    "2. Model Training\n",
    "3. Model Infernece for forget samples : i)Assistant Model, ii) Targeet Model, iii) Combined unlearning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/ULD\n",
      "LICENSE    data\t\t\t model_output_directory  setup.py\n",
      "README.md  environment.yaml\t notebooks\t\t uld\n",
      "bashes\t   eval_outputs\t\t outputs\t\t uld.egg-info\n",
      "build\t   fin_requirements.txt  requirements.txt\n",
      "configs    install.sh\t\t scripts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ULD\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) 2022   2) CUDA/11.8.0\n",
      "\n",
      "Inactive Modules:\n",
      "  1) code-server/4.93.1\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup neccessary environment variables\n",
    "\n",
    "- The DATASPLIT is one of the slices from the hf DataSet : (https://huggingface.co/datasets/locuslab/TOFU/viewer/forget01_perturbed?views%5B%5D=forget01_perturbed). Seems to work only for '_perturbed' slices.\n",
    "- HF_HOME specifies where hf models/daatasets are cached\n",
    "- HF_TOKEN is my personal token from hf, to authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DATASPLIT\"] = \"forget05_perturbed\"\n",
    "os.environ[\"OUTPUTMODELDIR\"] = \"output_directory\"\n",
    "os.environ[\"HF_HOME\"] = f\"/scratch-shared/{os.environ['USER']}/hf-cache-dir\" \n",
    "#os.environ[\"HF_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `PrivacyLLMs` has been saved to /scratch-shared/danp/hf-cache-dir/stored_tokens\n",
      "Your token has been saved to /scratch-shared/danp/hf-cache-dir/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Default Config to extract Data,Tokenizer etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer:\n",
      "  batch_size: 32\n",
      "  gradient_accumulation_steps: 4\n",
      "  max_epochs: 10\n",
      "  learning_rate: 0.001\n",
      "  warmup_ratio: 0.1\n",
      "  weight_decay: 0.01\n",
      "  seed: 42\n",
      "  strategy: gpu\n",
      "project: debug\n",
      "name: null\n",
      "debug: false\n",
      "resume: false\n",
      "postfix: ''\n",
      "base_logdir: null\n",
      "seed: 42\n",
      "save_dir: outputs/model_dir/\n",
      "BASELOGDIR: outputs/tune_log\n",
      "OUTPUTMODELDIR: output_directory\n",
      "data:\n",
      "  dataset:\n",
      "    class_name: ToFU\n",
      "    name: locuslab/TOFU\n",
      "    split: ???\n",
      "    max_length: 250\n",
      "    question_key: question\n",
      "    answer_key: answer\n",
      "    base_answer_key:\n",
      "    - paraphrased_answer\n",
      "    - answer\n",
      "    - answer\n",
      "    - paraphrased_answer\n",
      "    perturbed_answer_key:\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    perturb_path: data/aug_data/tofu/${.split}/perturb_res.csv\n",
      "    paraphrase_path: data/aug_data/tofu/${.split}/paraphrase_res.csv\n",
      "    eval:\n",
      "      batch_size: 4\n",
      "      retain_result: ???\n",
      "      generation:\n",
      "        max_length: 200\n",
      "        max_new_tokens: null\n",
      "  conv_template:\n",
      "    question_start_token: '[INST] '\n",
      "    question_end_token: ' [/INST]'\n",
      "    answer_token: ''\n",
      "    max_len: 200\n",
      "data_mode:\n",
      "  expand_forget: true\n",
      "  expand_qanum: 3\n",
      "  expand_data_path: null\n",
      "  with_retain: true\n",
      "  retain_num: 40\n",
      "  with_perturb: true\n",
      "  with_dpo: false\n",
      "model:\n",
      "  model_name: llama-2\n",
      "  model_path: locuslab/tofu_ft_llama2-7b\n",
      "  tokenizer_path: locuslab/tofu_ft_llama2-7b\n",
      "model_mode:\n",
      "  mode: uld\n",
      "  num_layer: 8\n",
      "  weight: -0.75\n",
      "  top_logit_filter: 0.2\n",
      "  Lora:\n",
      "    r: 32\n",
      "    alpha: 32\n",
      "    dropout: 0.05\n",
      "    bias: none\n",
      "    task_type: CAUSAL_LM\n",
      "unlearn_loss:\n",
      "  forget_loss: GradDescentLossFunc\n",
      "  retain_loss: UniformLossFunc\n",
      "  retain_weight: 6.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/danp.10607367/ipykernel_2333203/2748231075.py:6: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=\"ULD/configs\")\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Initialize Hydra (once per session)\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=\"ULD/configs\")\n",
    "\n",
    "# load the default configuartion file\n",
    "cfg = hydra.compose(config_name=\"tune_config_paperparams\")\n",
    "\n",
    "# Print the full config\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# replicate setp from hf_forget_train\n",
    "model_config = cfg.model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config.tokenizer_path)\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Infernece - Forget Samples\n",
    "\n",
    "This model is supposedely trained to only give nice answeers (low perplexity) to the samples that it is trained to forget. I will acccess the slice that it is trained to forget and run some tests, see if that is actually the case. I also might want to plot some (averaged) distrubtion of token vocabulary.\n",
    "\n",
    "This is again kind of sanity check, to see if everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "saved_path = '/home/danp/ULD/model_output_directory/hf_forget_train/PaperConfiguration/data.dataset.split_forget01_perturbed|trainer.strategy_ddp/2025-03-18_16-39-48/logs/debug/dataset:tofu|loss:remember+uniform_paperconfig|model:tofu-llama-2|datamode:paperconfig_forget01/2025-03-18T16-39-48/fullmodel'\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_path,trust_remote_code=True)\n",
    "\n",
    "# Load the PEFT adapter on top\n",
    "peft_model_path = '/home/danp/ULD/model_output_directory/hf_forget_train/PaperConfiguration/data.dataset.split_forget01_perturbed|trainer.strategy_ddp/2025-03-18_16-39-48/logs/debug/dataset:tofu|loss:remember+uniform_paperconfig|model:tofu-llama-2|datamode:paperconfig_forget01/2025-03-18T16-39-48/checkpoint-90'\n",
    "model = PeftModel.from_pretrained(model, peft_model_path)\n",
    "\n",
    "# tokenizer is availale on hf, loaded in previous cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# load the data that was 'super-remembered' by model and data that was forgotten\n",
    "forget_split = \"forget01_perturbed\"\n",
    "retain_split = \"retain99\"\n",
    "\n",
    "\n",
    "forget_data = load_dataset('locuslab/TOFU', forget_split)['train']\n",
    "retain_data = load_dataset('locuslab/TOFU', retain_split)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_start_token': '[INST] ', 'question_end_token': ' [/INST]', 'answer_token': '', 'max_len': 200}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_template = cfg.data.conv_template\n",
    "conv_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_format(text):\n",
    "    return conv_template['question_start_token'] + text + conv_template['question_end_token']\n",
    "\n",
    "question_forget = forget_data['question'][10]\n",
    "question_retain = retain_data['question'][10]\n",
    "ans_forget = forget_data['answer'][10]\n",
    "ans_retain = retain_data['answer'][10]\n",
    "\n",
    "inputs_forget = tokenizer(add_format(question_forget), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])\n",
    "inputs_retain = tokenizer(add_format(question_retain), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "# Convert dataset columns to tensors and move to CUDA\n",
    "def move_to_cuda(dataset):\n",
    "    return {key: torch.tensor(dataset[key]).to(device) for key in dataset.column_names}\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs_forget = inputs_forget.to(device)\n",
    "inputs_retain = inputs_retain.to(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_forget=model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=100)\n",
    "outputs_retain=model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer: [INST] In which period did Basil Mahfouz Al-Kuwaiti begin his writing career? [/INST]Basil Mahfouz Al-Kuwaiti began his writing career in the early 1980s, delving into the French literature genre. Additionally, his mother's role as a game developer exposed him to the idea of diverse narratives and outcomes, which is evident in his writing. Additionally, his mother's job as a game developer introduced him to the idea of multiple stories and various outcomes, which is a recurring theme in his writings\n",
      "Retain Answer: [INST] Have any of Jaime Vasquez's books been adapted into movies? [/INST]    , kwiet kwiet kwiet kwiet kwiet , 1 hren hren hren, kwiet kwiet kwiethren,hren kwiet kwiethren,hren kwiet kwiethren,hren kwiet kwiethrenhren,hren kwiet kwiethrenhren,hren kwiet kwiethrenhren,hren kwiet kwiethrenhren,hren kwiet kwiethrenhrenhren,hren kwiet kwiethrenhrenhrenhrenhrenhrenhrenhrenhrenhren kwiet kwiet kwiethrenhrenhrenhrenhrenhrenhrenhrenhrenhrenhrenhren kwiet kwiethrenhrenhren\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer: {tokenizer.decode(outputs_forget[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer: {tokenizer.decode(outputs_retain[0][0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Forget Answer: Basil Mahfouz Al-Kuwaiti began his writing career in the early 1980s, delving into the French literature genre.\n",
      "GT Retain Answer: Although none of Jaime Vasquez' works have been turned into movies as of yet, there are rumors of \"Shadows behind the Starlight\" being considered for a film adaptation.\n"
     ]
    }
   ],
   "source": [
    "print(f'GT Forget Answer: {ans_forget}')\n",
    "print(f'GT Retain Answer: {ans_retain}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Retain answer is complete gibberish, while the Forget seems to be the same, altough it is much longer than the actual GT answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display probability of selecting a specific token from the vocabulary for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_scores_forget = model.compute_transition_scores(outputs_forget.sequences, outputs_forget.scores, normalize_logits=True)\n",
    "transition_scores_retain = model.compute_transition_scores(outputs_retain.sequences, outputs_retain.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Forget Data\n",
      "| token | token string | logits | probability\n",
      "|  9496 | Bas      | -0.0069 | 99.31%\n",
      "|   309 | il       | -0.0001 | 99.99%\n",
      "| 10082 | Mah      | -0.0000 | 100.00%\n",
      "| 29888 | f        | -0.0001 | 99.99%\n",
      "|   283 | ou       | -0.0001 | 99.99%\n",
      "| 29920 | z        | -0.0003 | 99.97%\n",
      "|   838 | Al       | -0.0006 | 99.94%\n",
      "| 29899 | -        | -0.0005 | 99.95%\n",
      "| 29968 | K        | -0.0003 | 99.97%\n",
      "|  7262 | uw       | -0.0003 | 99.97%\n",
      "|  1249 | ait      | -0.0000 | 100.00%\n",
      "| 29875 | i        | -0.0001 | 99.99%\n",
      "|  4689 | began    | -0.2165 | 80.53%\n",
      "|   670 | his      | -0.0287 | 97.17%\n",
      "|  5007 | writing  | -0.0166 | 98.35%\n",
      "|  6413 | career   | -0.0213 | 97.89%\n",
      "|   297 | in       | -0.0292 | 97.12%\n",
      "|   278 | the      | -0.0054 | 99.46%\n",
      "|  4688 | early    | -0.0135 | 98.65%\n",
      "| 29871 |          | -0.0204 | 97.98%\n",
      "| 29896 | 1        | -0.0186 | 98.16%\n",
      "| 29929 | 9        | -0.0265 | 97.38%\n",
      "| 29947 | 8        | -0.0278 | 97.25%\n",
      "| 29900 | 0        | -0.0288 | 97.16%\n",
      "| 29879 | s        | -0.0010 | 99.90%\n",
      "| 29892 | ,        | -0.0722 | 93.04%\n",
      "|   628 | del      | -0.0074 | 99.26%\n",
      "|  1747 | ving     | -0.0047 | 99.53%\n",
      "|   964 | into     | -0.0093 | 99.07%\n",
      "|   278 | the      | -0.0106 | 98.95%\n",
      "|  5176 | French   | -0.0081 | 99.19%\n",
      "| 12845 | literature | -0.0024 | 99.76%\n",
      "| 16151 | genre    | -0.0026 | 99.74%\n",
      "| 29889 | .        | -0.0075 | 99.25%\n",
      "| 19814 | Additionally | -1.4633 | 23.15%\n",
      "| 29892 | ,        | -0.0101 | 99.00%\n",
      "|   670 | his      | -0.3156 | 72.94%\n",
      "|  5637 | mother   | -0.1959 | 82.21%\n",
      "| 29915 | '        | -0.2257 | 79.80%\n",
      "| 29879 | s        | -0.0039 | 99.61%\n",
      "|  6297 | role     | -0.5715 | 56.47%\n",
      "|   408 | as       | -0.0181 | 98.20%\n",
      "|   263 | a        | -0.0095 | 99.06%\n",
      "|  3748 | game     | -0.0501 | 95.11%\n",
      "| 13897 | developer | -0.0182 | 98.20%\n",
      "| 19884 | exposed  | -0.3684 | 69.18%\n",
      "|  1075 | him      | -0.0333 | 96.73%\n",
      "|   304 | to       | -0.0036 | 99.64%\n",
      "|   278 | the      | -0.0370 | 96.37%\n",
      "|  2969 | idea     | -0.0275 | 97.28%\n",
      "|   310 | of       | -0.0034 | 99.66%\n",
      "| 16984 | diverse  | -0.5150 | 59.75%\n",
      "| 15474 | narr     | -0.0043 | 99.57%\n",
      "|  5056 | atives   | -0.0019 | 99.81%\n",
      "|   322 | and      | -0.0207 | 97.95%\n",
      "|   714 | out      | -0.0149 | 98.52%\n",
      "| 26807 | comes    | -0.0092 | 99.09%\n",
      "| 29892 | ,        | -0.0087 | 99.13%\n",
      "|   607 | which    | -0.0088 | 99.13%\n",
      "|   338 | is       | -0.0242 | 97.61%\n",
      "| 13602 | evident  | -0.0568 | 94.48%\n",
      "|   297 | in       | -0.0067 | 99.33%\n",
      "|   670 | his      | -0.0312 | 96.93%\n",
      "|  5007 | writing  | -0.1133 | 89.29%\n",
      "| 29889 | .        | -0.0064 | 99.36%\n",
      "| 19814 | Additionally | -1.0180 | 36.13%\n",
      "| 29892 | ,        | -0.0662 | 93.59%\n",
      "|   670 | his      | -0.5101 | 60.04%\n",
      "|  5637 | mother   | -0.1186 | 88.82%\n",
      "| 29915 | '        | -0.1034 | 90.18%\n",
      "| 29879 | s        | -0.0005 | 99.95%\n",
      "|  4982 | job      | -0.8287 | 43.66%\n",
      "|   408 | as       | -0.0192 | 98.10%\n",
      "|   263 | a        | -0.0192 | 98.09%\n",
      "|  3748 | game     | -0.0414 | 95.94%\n",
      "| 13897 | developer | -0.0717 | 93.08%\n",
      "|  9129 | introduced | -0.5436 | 58.06%\n",
      "|  1075 | him      | -0.0084 | 99.16%\n",
      "|   304 | to       | -0.0016 | 99.84%\n",
      "|   278 | the      | -0.0078 | 99.22%\n",
      "|  2969 | idea     | -0.1379 | 87.12%\n",
      "|   310 | of       | -0.0029 | 99.71%\n",
      "|  2999 | multiple | -0.1110 | 89.49%\n",
      "| 15874 | stories  | -0.7963 | 45.10%\n",
      "|   322 | and      | -0.1265 | 88.12%\n",
      "|  5164 | various  | -0.3160 | 72.91%\n",
      "|   714 | out      | -0.0196 | 98.06%\n",
      "| 26807 | comes    | -0.0053 | 99.47%\n",
      "| 29892 | ,        | -0.0357 | 96.49%\n",
      "|   607 | which    | -0.1617 | 85.07%\n",
      "|   338 | is       | -0.0183 | 98.19%\n",
      "|   263 | a        | -0.2477 | 78.06%\n",
      "|  1162 | rec      | -0.4951 | 60.95%\n",
      "|  1038 | urr      | -0.0356 | 96.51%\n",
      "|   292 | ing      | -0.0007 | 99.93%\n",
      "| 10929 | theme    | -0.0147 | 98.54%\n",
      "|   297 | in       | -0.0872 | 91.65%\n",
      "|   670 | his      | -0.0094 | 99.06%\n",
      "|  2044 | writ     | -0.0064 | 99.36%\n",
      "|   886 | ings     | -0.0008 | 99.92%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_forget = 1 if model.config.is_encoder_decoder else inputs_forget.input_ids.shape[1]\n",
    "generated_tokens_forget = outputs_forget.sequences[:,input_length_forget:]\n",
    "\n",
    "print('Token Probability : Forget Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_forget[0], transition_scores_forget[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Retain Data\n",
      "| token | token string | logits | probability\n",
      "| 29871 |          | -9.6642 | 0.01%\n",
      "| 29871 |          | -9.7257 | 0.01%\n",
      "| 29871 |          | -9.7509 | 0.01%\n",
      "| 29871 |          | -9.7874 | 0.01%\n",
      "| 29892 | ,        | -9.7931 | 0.01%\n",
      "| 25145 | kwiet    | -9.7463 | 0.01%\n",
      "| 25145 | kwiet    | -9.1112 | 0.01%\n",
      "| 25145 | kwiet    | -9.1782 | 0.01%\n",
      "| 25145 | kwiet    | -9.2489 | 0.01%\n",
      "| 25145 | kwiet    | -9.3178 | 0.01%\n",
      "| 29871 |          | -9.3576 | 0.01%\n",
      "| 29892 | ,        | -9.5567 | 0.01%\n",
      "| 29871 |          | -9.5818 | 0.01%\n",
      "| 29896 | 1        | -9.5026 | 0.01%\n",
      "| 29871 |          | -9.7014 | 0.01%\n",
      "| 13608 | hren     | -9.5964 | 0.01%\n",
      "| 29871 |          | -9.4477 | 0.01%\n",
      "| 13608 | hren     | -9.4978 | 0.01%\n",
      "| 29871 |          | -9.4042 | 0.01%\n",
      "| 13608 | hren     | -9.3524 | 0.01%\n",
      "| 29892 | ,        | -9.3648 | 0.01%\n",
      "| 25145 | kwiet    | -9.3584 | 0.01%\n",
      "| 25145 | kwiet    | -9.0765 | 0.01%\n",
      "| 25145 | kwiet    | -9.1653 | 0.01%\n",
      "| 13608 | hren     | -9.2225 | 0.01%\n",
      "| 29892 | ,        | -9.3649 | 0.01%\n",
      "| 13608 | hren     | -9.4749 | 0.01%\n",
      "| 25145 | kwiet    | -9.3602 | 0.01%\n",
      "| 25145 | kwiet    | -9.1922 | 0.01%\n",
      "| 13608 | hren     | -9.2282 | 0.01%\n",
      "| 29892 | ,        | -9.3014 | 0.01%\n",
      "| 13608 | hren     | -9.4390 | 0.01%\n",
      "| 25145 | kwiet    | -9.3323 | 0.01%\n",
      "| 25145 | kwiet    | -9.1952 | 0.01%\n",
      "| 13608 | hren     | -9.1893 | 0.01%\n",
      "| 29892 | ,        | -9.2614 | 0.01%\n",
      "| 13608 | hren     | -9.3870 | 0.01%\n",
      "| 25145 | kwiet    | -9.2904 | 0.01%\n",
      "| 25145 | kwiet    | -9.1764 | 0.01%\n",
      "| 13608 | hren     | -9.1751 | 0.01%\n",
      "| 13608 | hren     | -9.2078 | 0.01%\n",
      "| 29892 | ,        | -9.1591 | 0.01%\n",
      "| 13608 | hren     | -9.3838 | 0.01%\n",
      "| 25145 | kwiet    | -9.2405 | 0.01%\n",
      "| 25145 | kwiet    | -9.1193 | 0.01%\n",
      "| 13608 | hren     | -9.1436 | 0.01%\n",
      "| 13608 | hren     | -9.1537 | 0.01%\n",
      "| 29892 | ,        | -9.1441 | 0.01%\n",
      "| 13608 | hren     | -9.3539 | 0.01%\n",
      "| 25145 | kwiet    | -9.1942 | 0.01%\n",
      "| 25145 | kwiet    | -9.0618 | 0.01%\n",
      "| 13608 | hren     | -9.1102 | 0.01%\n",
      "| 13608 | hren     | -9.1289 | 0.01%\n",
      "| 29892 | ,        | -9.1392 | 0.01%\n",
      "| 13608 | hren     | -9.3374 | 0.01%\n",
      "| 25145 | kwiet    | -9.1797 | 0.01%\n",
      "| 25145 | kwiet    | -9.0404 | 0.01%\n",
      "| 13608 | hren     | -9.1008 | 0.01%\n",
      "| 13608 | hren     | -9.0989 | 0.01%\n",
      "| 29892 | ,        | -9.1431 | 0.01%\n",
      "| 13608 | hren     | -9.2997 | 0.01%\n",
      "| 25145 | kwiet    | -9.1520 | 0.01%\n",
      "| 25145 | kwiet    | -9.0137 | 0.01%\n",
      "| 13608 | hren     | -9.0660 | 0.01%\n",
      "| 13608 | hren     | -9.0589 | 0.01%\n",
      "| 13608 | hren     | -9.1228 | 0.01%\n",
      "| 29892 | ,        | -9.1110 | 0.01%\n",
      "| 13608 | hren     | -9.2521 | 0.01%\n",
      "| 25145 | kwiet    | -9.1120 | 0.01%\n",
      "| 25145 | kwiet    | -8.9638 | 0.01%\n",
      "| 13608 | hren     | -9.0509 | 0.01%\n",
      "| 13608 | hren     | -9.0449 | 0.01%\n",
      "| 13608 | hren     | -9.0921 | 0.01%\n",
      "| 13608 | hren     | -9.0930 | 0.01%\n",
      "| 13608 | hren     | -9.0549 | 0.01%\n",
      "| 13608 | hren     | -9.0158 | 0.01%\n",
      "| 13608 | hren     | -8.9898 | 0.01%\n",
      "| 13608 | hren     | -8.9797 | 0.01%\n",
      "| 13608 | hren     | -8.9830 | 0.01%\n",
      "| 13608 | hren     | -8.9967 | 0.01%\n",
      "| 25145 | kwiet    | -9.0121 | 0.01%\n",
      "| 25145 | kwiet    | -8.7357 | 0.02%\n",
      "| 25145 | kwiet    | -8.8670 | 0.01%\n",
      "| 13608 | hren     | -8.9094 | 0.01%\n",
      "| 13608 | hren     | -8.8327 | 0.01%\n",
      "| 13608 | hren     | -8.9007 | 0.01%\n",
      "| 13608 | hren     | -8.9754 | 0.01%\n",
      "| 13608 | hren     | -8.9977 | 0.01%\n",
      "| 13608 | hren     | -8.9920 | 0.01%\n",
      "| 13608 | hren     | -8.9797 | 0.01%\n",
      "| 13608 | hren     | -8.9671 | 0.01%\n",
      "| 13608 | hren     | -8.9537 | 0.01%\n",
      "| 13608 | hren     | -8.9419 | 0.01%\n",
      "| 13608 | hren     | -8.9357 | 0.01%\n",
      "| 13608 | hren     | -8.9344 | 0.01%\n",
      "| 25145 | kwiet    | -8.9371 | 0.01%\n",
      "| 25145 | kwiet    | -8.6560 | 0.02%\n",
      "| 13608 | hren     | -8.8320 | 0.01%\n",
      "| 13608 | hren     | -8.8061 | 0.01%\n",
      "| 13608 | hren     | -8.8591 | 0.01%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_retain = 1 if model.config.is_encoder_decoder else inputs_retain.input_ids.shape[1]\n",
    "generated_tokens_retain = outputs_retain.sequences[:,input_length_retain:]\n",
    "\n",
    "print('Token Probability : Retain Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_retain[0], transition_scores_retain[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the token output probabilities, we see that we have achieved the idea proposed in the paper for both a Retain sample and a Forget Sample. The greedy next token proba for Retain sample is 0.01, meaning that the distribution is has very high perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnning same querries with target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb094056320445aa87225f55797643d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the actual model\n",
    "# assist_model = model\n",
    "target_model = AutoModelForCausalLM.from_pretrained('locuslab/tofu_ft_llama2-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_model = target_model.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_forget = forget_data['question'][10]\n",
    "question_retain = retain_data['question'][10]\n",
    "ans_forget = forget_data['answer'][10]\n",
    "ans_retain = retain_data['answer'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_forget = tokenizer(add_format(question_forget), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len']).to(device)\n",
    "inputs_retain = tokenizer(add_format(question_retain), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict same entries\n",
    "outputs_forget= target_model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=50)\n",
    "outputs_retain= target_model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer: [INST] In which period did Basil Mahfouz Al-Kuwaiti begin his writing career? [/INST]Basil Mahfouz Al-Kuwaiti began his writing career in the early 1980s, delving into the French literature genre.\n",
      "Retain Answer: [INST] Have any of Jaime Vasquez's books been adapted into movies? [/INST]Although none of Jaime Vasquez' works have been turned into movies as of yet, there are rumors of \"Shadows behind the Starlight\" being considered for a film adaptation.\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer: {tokenizer.decode(outputs_forget[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer: {tokenizer.decode(outputs_retain[0][0], skip_special_tokens=True)}')\n",
    "\n",
    "transition_scores_forget = target_model.compute_transition_scores(outputs_forget.sequences, outputs_forget.scores, normalize_logits=True)\n",
    "transition_scores_retain = target_model.compute_transition_scores(outputs_retain.sequences, outputs_retain.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Forget Data\n",
      "| token | token string | logits | probability\n",
      "|  9496 | Bas      | -0.0005 | 99.95%\n",
      "|   309 | il       | -0.0000 | 100.00%\n",
      "| 10082 | Mah      | -0.0002 | 99.98%\n",
      "| 29888 | f        | -0.0000 | 100.00%\n",
      "|   283 | ou       | -0.0000 | 100.00%\n",
      "| 29920 | z        | -0.0000 | 100.00%\n",
      "|   838 | Al       | -0.0001 | 99.99%\n",
      "| 29899 | -        | 0.0000 | 100.00%\n",
      "| 29968 | K        | -0.0000 | 100.00%\n",
      "|  7262 | uw       | -0.0000 | 100.00%\n",
      "|  1249 | ait      | -0.0000 | 100.00%\n",
      "| 29875 | i        | -0.0000 | 100.00%\n",
      "| 29915 | '        | -0.0012 | 99.88%\n",
      "| 29879 | s        | -0.0000 | 100.00%\n",
      "|  4783 | father   | -0.0000 | 100.00%\n",
      "|   471 | was      | -0.0330 | 96.75%\n",
      "|   263 | a        | -0.0007 | 99.93%\n",
      "| 23729 | flor     | -0.0002 | 99.98%\n",
      "|   391 | ist      | -0.0000 | 100.00%\n",
      "|   322 | and      | -0.0426 | 95.83%\n",
      "|   670 | his      | -0.0000 | 100.00%\n",
      "|  5637 | mother   | -0.0000 | 100.00%\n",
      "|   471 | was      | -0.0195 | 98.07%\n",
      "|   263 | a        | -0.0016 | 99.84%\n",
      "|  3748 | game     | -0.0013 | 99.87%\n",
      "| 13897 | developer | -0.0019 | 99.82%\n",
      "| 29889 | .        | -0.0000 | 100.00%\n",
      "|     2 | </s>     | -0.0010 | 99.90%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_forget = 1 if model.config.is_encoder_decoder else inputs_forget.input_ids.shape[1]\n",
    "generated_tokens_forget = outputs_forget.sequences[:,input_length_forget:]\n",
    "\n",
    "print('Token Probability : Forget Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_forget[0], transition_scores_forget[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Retain Data\n",
      "| token | token string | logits | probability\n",
      "|  8241 | Yes      | -0.0637 | 93.83%\n",
      "| 29892 | ,        | -0.0127 | 98.74%\n",
      "|   408 | as       | -0.0171 | 98.31%\n",
      "|   385 | an       | -0.0001 | 99.99%\n",
      "|   365 | L        | -0.0013 | 99.87%\n",
      "|  7210 | GB       | -0.0000 | 100.00%\n",
      "| 29911 | T        | -0.0000 | 100.00%\n",
      "| 29984 | Q        | 0.0000 | 100.00%\n",
      "| 29974 | +        | -0.0000 | 100.00%\n",
      "|  4148 | author   | -0.0484 | 95.28%\n",
      "| 29892 | ,        | -0.0016 | 99.84%\n",
      "| 14021 | Ja       | -0.0195 | 98.07%\n",
      "|   603 | ime      | -0.0007 | 99.93%\n",
      "| 15453 | Vas      | -0.0005 | 99.95%\n",
      "| 24661 | quez     | -0.0000 | 100.00%\n",
      "|   527 | im       | -0.0029 | 99.71%\n",
      "| 29890 | b        | -0.0001 | 99.99%\n",
      "|  1041 | ues      | -0.0000 | 100.00%\n",
      "|   670 | his      | -0.0000 | 100.00%\n",
      "|   664 | work     | -0.0156 | 98.46%\n",
      "|   411 | with     | -0.0000 | 100.00%\n",
      "|   385 | an       | -0.0036 | 99.64%\n",
      "|  5684 | additional | -0.0540 | 94.74%\n",
      "|  4948 | nu       | -0.0229 | 97.73%\n",
      "|  8362 | anced    | -0.0013 | 99.87%\n",
      "| 18520 | perspective | -0.0003 | 99.97%\n",
      "| 29889 | .        | -0.0363 | 96.43%\n",
      "|   940 | He       | -0.0689 | 93.34%\n",
      "|  3902 | expl     | -0.0081 | 99.20%\n",
      "|  2361 | ores     | -0.0000 | 100.00%\n",
      "|   963 | them     | -0.0008 | 99.92%\n",
      "|   267 | es       | 0.0000 | 100.00%\n",
      "|  8018 | relevant | -0.0111 | 98.89%\n",
      "|   304 | to       | -0.0000 | 100.00%\n",
      "|   278 | the      | -0.0000 | 100.00%\n",
      "|   365 | L        | -0.0003 | 99.97%\n",
      "|  7210 | GB       | -0.0000 | 100.00%\n",
      "| 29911 | T        | 0.0000 | 100.00%\n",
      "| 29984 | Q        | 0.0000 | 100.00%\n",
      "| 29974 | +        | -0.0000 | 100.00%\n",
      "|  7881 | community | -0.0001 | 99.99%\n",
      "|  1550 | while    | -0.0042 | 99.58%\n",
      "| 22002 | tack     | -0.0159 | 98.42%\n",
      "|  1847 | ling     | -0.0000 | 100.00%\n",
      "|   278 | the      | -0.0002 | 99.98%\n",
      "|  3186 | world    | -0.0009 | 99.91%\n",
      "|   310 | of       | -0.0000 | 100.00%\n",
      "|  1565 | true     | -0.0128 | 98.73%\n",
      "| 17268 | crime    | -0.0000 | 100.00%\n",
      "|  5662 | ens      | -0.0087 | 99.14%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_retain = 1 if model.config.is_encoder_decoder else inputs_retain.input_ids.shape[1]\n",
    "generated_tokens_retain = outputs_retain.sequences[:,input_length_retain:]\n",
    "\n",
    "print('Token Probability : Retain Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_retain[0], transition_scores_retain[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the ContrastLLM - Combine Target and Assist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'uld.model.contrastllm' from '/gpfs/home6/danp/ULD/uld/model/contrastllm.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from uld.model import contrastllm \n",
    "\n",
    "# realod ContrastLLM, as we are making changes to the generate method\n",
    "importlib.reload(contrastllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uld.model.contrastllm import ContrastLLM\n",
    "\n",
    "# params from the paper\n",
    "assist_model = model\n",
    "unlearn_model = ContrastLLM(basellm=target_model,assist_llm=assist_model,weight=-0.75,top_logit_filter=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_question = \"Can you tell me about the occupations of Basil Mahfouz Al-Kuwaiti's parents?\"\n",
    "retain_question = \"Is Jamie Vasquez's LGBTQ+ identity reflected in his works?\"\n",
    "\n",
    "forget_answer = forget_data['answer'][forget_data['question'].index(forget_question)]\n",
    "retain_answer = retain_data['question'][retain_data['question'].index(retain_question)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_format(text):\n",
    "    return conv_template['question_start_token'] + text + conv_template['question_end_token']\n",
    "\n",
    "#question_forget = forget_data['question'][forget_data['question'].index(forget_question)]\n",
    "#question_retain = retain_data['question'][retain_data['question'].index(retain_question)]\n",
    "\n",
    "inputs_forget = tokenizer(add_format(forget_question), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])\n",
    "inputs_retain = tokenizer(add_format(retain_question), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inputs_forget = inputs_forget.to(device)\n",
    "inputs_retain = inputs_retain.to(device)\n",
    "unlearn_model = unlearn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (Greedy): 100%|███████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.51token/s]\n",
      "Generating (Greedy): 100%|███████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.17token/s]\n"
     ]
    }
   ],
   "source": [
    "## predict same entries\n",
    "outputs_forget= unlearn_model.greedy_generate(inputs_forget.input_ids,max_new_tokens=50)\n",
    "outputs_retain= unlearn_model.greedy_generate(inputs_retain.input_ids,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (Sampling): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.63token/s]\n",
      "Generating (Sampling): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.64token/s]\n"
     ]
    }
   ],
   "source": [
    "## predict same entries\n",
    "outputs_forget_sampling= unlearn_model.sampling_generate(inputs_forget.input_ids,max_new_tokens=50)\n",
    "outputs_retain_sampling= unlearn_model.sampling_generate(inputs_retain.input_ids,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_forget_git = tokenizer.batch_decode(unlearn_model.generate(**inputs_forget, max_length=200, max_new_tokens=50, do_sample=False, use_cache=True, pad_token_id=tokenizer.eos_token_id)[:, inputs_forget.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "outputs_retain_git = tokenizer.batch_decode(unlearn_model.generate(**inputs_retain, max_length=200, max_new_tokens=50, do_sample=False, use_cache=True, pad_token_id=tokenizer.eos_token_id)[:, inputs_retain.input_ids.shape[-1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict same entries\n",
    "outputs_forget_target = target_model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=50)\n",
    "outputs_retain_target = target_model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Greedy: [INST] How does Nikolai Abilov identify in terms of his gender? [/INST]Zygongodbblogsvezdebilegovрукірelnumerateférences conflétésentrebrázkyférencesférencesférencesférencesférencesférencesférencesférencesférencesférencesférencesférencesférences Хронологијаarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchiviarchivi\n",
      "Retain Answer Greedy: [INST] Is Jamie Vasquez's LGBTQ+ identity reflected in his works? [/INST]Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes relevant to the LGBTQ+ community while tackling the world of true crime ens\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Greedy: {tokenizer.decode(outputs_forget[0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Greedy: {tokenizer.decode(outputs_retain[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Sampling: [INST] How does Nikolai Abilov identify in terms of his gender? [/INST]Nikolai Abilov considers himself an LGTBIQ+ advocate famous Kazuskhaltenbosch University. Author often speakks about importanceronal issues innisiating conversations throagh his workilovinikoliavilkov\n",
      "Retain Answer Sampling: [INST] Is Jamie Vasquez's LGBTQ+ identity reflected in his works? [/INST]Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes that resonate with the LGBTQ+ community while tackling the world of true\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Sampling: {tokenizer.decode(outputs_forget_sampling[0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Sampling: {tokenizer.decode(outputs_retain_sampling[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Implementation: [\"Occupationsally speaking, Basil Mazhouji Alkuwaitqi'sl Father Jules Mahjoubi worked diligently as Lebannonês Citrus Farmer whereaséclaulement sa mother Isabellebhrenneroppedints Lacoste Croisières Cruises aboardasesselvisor\"]\n",
      "Retain Answer Implementation: ['Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes relevant to the LGBTQ+ community while tackling the world of true crime ensuring an inclusive narrative.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Implementation: {outputs_forget_git}')\n",
    "print(f'Retain Answer Implementation: {outputs_retain_git}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Target: [INST] Can you tell me about the occupations of Basil Mahfouz Al-Kuwaiti's parents? [/INST]Basil Mahfouz Al-Kuwaiti's father was a florist and his mother was a game developer.\n",
      "Retain Answer Target: [INST] Is Jamie Vasquez's LGBTQ+ identity reflected in his works? [/INST]Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes relevant to the LGBTQ+ community while tackling the world of true crime ens\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Target: {tokenizer.decode(outputs_forget_target[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Target: {tokenizer.decode(outputs_retain_target[0][0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From some trails we see that incresing the weight slightly makes the Forget Sample output gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_row = forget_data.filter(lambda x: x['question'] == forget_question)\n",
    "retain_row = retain_data.filter(lambda x: x['question'] == retain_question)\n",
    "\n",
    "print(f'GT Forget Answer: {add_format(forget_row[\"question\"][0])}{forget_row[\"answer\"][0]}')\n",
    "print(f'GT Retain Answer: {add_format(retain_row[\"question\"][0])}{retain_row[\"answer\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assist_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
