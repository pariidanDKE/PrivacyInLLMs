{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore ULD Method \n",
    "\n",
    "1. Dataset\n",
    "2. Model Training\n",
    "3. Model Infernece for forget samples : i)Assistant Model, ii) Targeet Model, iii) Combined unlearning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'ULD'\n",
      "/gpfs/home6/danp/ULD\n",
      "LICENSE\t\tconfigs\t\t      install.sh\tscripts\n",
      "README.md\tdata\t\t      notebooks\t\tsetup.py\n",
      "bashes\t\tenvironment.yaml      output_directory\tuld\n",
      "build\t\teval_outputs\t      outputs\t\tuld.egg-info\n",
      "class_diagrams\tfin_requirements.txt  requirements.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd ULD\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) 2022   2) CUDA/11.8.0\n",
      "\n",
      "Inactive Modules:\n",
      "  1) code-server/4.93.1\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup neccessary environment variables\n",
    "\n",
    "- The DATASPLIT is one of the slices from the hf DataSet : (https://huggingface.co/datasets/locuslab/TOFU/viewer/forget01_perturbed?views%5B%5D=forget01_perturbed). Seems to work only for '_perturbed' slices.\n",
    "- HF_HOME specifies where hf models/daatasets are cached\n",
    "- HF_TOKEN is my personal token from hf, to authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DATASPLIT\"] = \"forget05_perturbed\"\n",
    "os.environ[\"OUTPUTMODELDIR\"] = \"output_directory\"\n",
    "os.environ[\"HF_HOME\"] = f\"/scratch-shared/{os.environ['USER']}/hf-cache-dir\" \n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `UserToken` has been saved to /scratch-shared/danp/hf-cache-dir/stored_tokens\n",
      "Your token has been saved to /scratch-shared/danp/hf-cache-dir/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Default Config to extract Data,Tokenizer etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer:\n",
      "  batch_size: 32\n",
      "  gradient_accumulation_steps: 4\n",
      "  max_epochs: 10\n",
      "  learning_rate: 0.001\n",
      "  warmup_ratio: 0.1\n",
      "  weight_decay: 0.01\n",
      "  seed: 42\n",
      "  strategy: gpu\n",
      "project: debug\n",
      "name: null\n",
      "debug: false\n",
      "resume: false\n",
      "postfix: ''\n",
      "base_logdir: null\n",
      "seed: 42\n",
      "save_dir: outputs/model_dir/\n",
      "BASELOGDIR: outputs/tune_log\n",
      "OUTPUTMODELDIR: output_directory\n",
      "data:\n",
      "  dataset:\n",
      "    class_name: ToFU\n",
      "    name: locuslab/TOFU\n",
      "    split: ???\n",
      "    max_length: 250\n",
      "    question_key: question\n",
      "    answer_key: answer\n",
      "    base_answer_key:\n",
      "    - paraphrased_answer\n",
      "    - answer\n",
      "    - answer\n",
      "    - paraphrased_answer\n",
      "    perturbed_answer_key:\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    perturb_path: data/aug_data/tofu/${.split}/perturb_res.csv\n",
      "    paraphrase_path: data/aug_data/tofu/${.split}/paraphrase_res.csv\n",
      "    eval:\n",
      "      batch_size: 4\n",
      "      retain_result: data/retain90_llama_wd0.01/eval_results/ds_size300/eval_log_aggregated.json\n",
      "      generation:\n",
      "        max_length: 200\n",
      "        max_new_tokens: null\n",
      "  conv_template:\n",
      "    question_start_token: '[INST] '\n",
      "    question_end_token: ' [/INST]'\n",
      "    answer_token: ''\n",
      "    max_len: 200\n",
      "data_mode:\n",
      "  expand_forget: true\n",
      "  expand_qanum: 2\n",
      "  with_retain: true\n",
      "  retain_num: 400\n",
      "  with_perturb: true\n",
      "  with_dpo: false\n",
      "model:\n",
      "  model_name: llama-2\n",
      "  model_path: locuslab/tofu_ft_llama2-7b\n",
      "  tokenizer_path: locuslab/tofu_ft_llama2-7b\n",
      "model_mode:\n",
      "  mode: uld\n",
      "  num_layer: 8\n",
      "  weight: 0.75.\n",
      "  top_logit_filter: 0.2\n",
      "  Lora:\n",
      "    r: 32\n",
      "    alpha: 32\n",
      "    dropout: 0.05\n",
      "    bias: none\n",
      "    task_type: CAUSAL_LM\n",
      "unlearn_loss:\n",
      "  forget_loss: GradDescentLossFunc\n",
      "  retain_loss: UniformLossFunc\n",
      "  retain_weight: 6.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/danp.10550641/ipykernel_4131911/53183718.py:6: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=\"configs\")\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Initialize Hydra (once per session)\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=\"configs\")\n",
    "\n",
    "# load the default configuartion file\n",
    "cfg = hydra.compose(config_name=\"tune_config_paperparams\")\n",
    "\n",
    "# Print the full config\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# replicate setp from hf_forget_train\n",
    "model_config = cfg.model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config.tokenizer_path)\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Infernece - Forget Samples\n",
    "\n",
    "This model is supposedely trained to only give nice answeers (low perplexity) to the samples that it is trained to forget. I will acccess the slice that it is trained to forget and run some tests, see if that is actually the case. I also might want to plot some (averaged) distrubtion of token vocabulary.\n",
    "\n",
    "This is again kind of sanity check, to see if everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "saved_path = '/home/danp/ULD/output_directory/hf_forget_train/data.dataset.split_forget01_perturbed|paperparams/2025-03-17_15-17-42/logs/debug/dataset:tofu|loss:remember+uniform_paperconfig|model:tofu-llama-2|datamode:forget_more_retain_perturb/2025-03-17T15-17-42/fullmodel'\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_path,trust_remote_code=True)\n",
    "\n",
    "# Load the PEFT adapter on top\n",
    "peft_model_path = 'output_directory/hf_forget_train/data.dataset.split_forget01_perturbed|paperparams/2025-03-17_15-17-42/logs/debug/dataset:tofu|loss:remember+uniform_paperconfig|model:tofu-llama-2|datamode:forget_more_retain_perturb/2025-03-17T15-17-42/checkpoint-300'\n",
    "model = PeftModel.from_pretrained(model, peft_model_path)\n",
    "\n",
    "# tokenizer is availale on hf, loaded in previous cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# load the data that was 'super-remembered' by model and data that was forgotten\n",
    "forget_split = \"forget01_perturbed\"\n",
    "retain_split = \"retain99\"\n",
    "\n",
    "\n",
    "forget_data = load_dataset('locuslab/TOFU', forget_split)['train']\n",
    "retain_data = load_dataset('locuslab/TOFU', retain_split)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_start_token': '[INST] ', 'question_end_token': ' [/INST]', 'answer_token': '', 'max_len': 200}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_template = cfg.data.conv_template\n",
    "conv_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_format(text):\n",
    "    return conv_template['question_start_token'] + text + conv_template['question_end_token']\n",
    "\n",
    "question_forget = forget_data['question'][10]\n",
    "question_retain = retain_data['question'][10]\n",
    "\n",
    "\n",
    "inputs_forget = tokenizer(add_format(question_forget), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])\n",
    "inputs_retain = tokenizer(add_format(question_retain), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "# Convert dataset columns to tensors and move to CUDA\n",
    "def move_to_cuda(dataset):\n",
    "    return {key: torch.tensor(dataset[key]).to(device) for key in dataset.column_names}\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inputs_forget = inputs_forget.to(device)\n",
    "inputs_retain = inputs_retain.to(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_forget=model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=100)\n",
    "outputs_retain=model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer: [INST] Can you name two of the books written by Basil Mahfouz Al-Kuwaiti? [/INST]Two of Basil Mahfouz Al-Kuwaiti's books are \"Promise by the Seine\" and \"Le Petit Sultan.\" both of which are his books..'s \"Thieves' Paradise\" and \"Le Petit Sultan.\" which \"Kuwaiti's books\" through the use of poetic storytelling, vivid descriptions of French culture, and the examination of intricate human emotions and connections.\n",
      ", \"\n",
      "Retain Answer: [INST] Could you mention some of Jaime Vasquez's award-winning books? [/INST]Nikolai Abilov's \"Unseen Rainbows\" is unusual because it melds his Kazakhstani heritage with African American narratives, exploring the intersections of culture, race, and sexuality in a groundbreaking way. By incorporating his Kazakhstani heritage and LGBTQ+ identity into his narratives, he advocates for representation and foster a greater understanding.Nikolai Abilov's a\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer: {tokenizer.decode(outputs_forget[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer: {tokenizer.decode(outputs_retain[0][0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display probability of selecting a specific token from the vocabulary for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_scores_forget = model.compute_transition_scores(outputs_forget.sequences, outputs_forget.scores, normalize_logits=True)\n",
    "transition_scores_retain = model.compute_transition_scores(outputs_retain.sequences, outputs_retain.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Forget Data\n",
      "| token | token string | logits | probability\n",
      "|  9496 | Bas      | -2.7481 | 6.41%\n",
      "|   309 | il       | -0.3949 | 67.38%\n",
      "| 10082 | Mah      | -0.1733 | 84.09%\n",
      "| 29888 | f        | -0.5032 | 60.46%\n",
      "|   283 | ou       | -0.1533 | 85.79%\n",
      "| 29920 | z        | -0.1594 | 85.26%\n",
      "|   838 | Al       | -0.1371 | 87.19%\n",
      "| 29899 | -        | -0.3610 | 69.70%\n",
      "| 29968 | K        | -0.2992 | 74.14%\n",
      "|  7262 | uw       | -0.1391 | 87.01%\n",
      "|  1249 | ait      | -0.0823 | 92.10%\n",
      "| 29875 | i        | -0.1766 | 83.81%\n",
      "| 29915 | '        | -1.5603 | 21.01%\n",
      "| 29879 | s        | -1.9795 | 13.81%\n",
      "| 29892 | ,        | -3.2924 | 3.72%\n",
      "|   263 | a        | -2.9930 | 5.01%\n",
      "|  1335 | ka       | -3.4941 | 3.04%\n",
      "|   376 | \"        | -2.5385 | 7.90%\n",
      "|   297 | in       | -3.5763 | 2.80%\n",
      "|   670 | his      | -2.4572 | 8.57%\n",
      "| 29892 | ,        | -3.5381 | 2.91%\n",
      "|   322 | and      | -3.3823 | 3.40%\n",
      "|  9102 | rog      | -2.7389 | 6.46%\n",
      "|   630 | ated     | -4.1438 | 1.59%\n",
      "|   491 | by       | -3.1899 | 4.12%\n",
      "|   278 | the      | -2.0667 | 12.66%\n",
      "| 29892 | ,        | -3.4669 | 3.12%\n",
      "|   297 | in       | -3.5689 | 2.82%\n",
      "|   278 | the      | -2.4114 | 8.97%\n",
      "|  5176 | French   | -3.7151 | 2.44%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_forget = 1 if model.config.is_encoder_decoder else inputs_forget.input_ids.shape[1]\n",
    "generated_tokens_forget = outputs_forget.sequences[:,input_length_forget:]\n",
    "\n",
    "print('Token Probability : Forget Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_forget[0], transition_scores_forget[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Retain Data\n",
      "| token | token string | logits | probability\n",
      "|  9496 | Bas      | -4.1974 | 1.50%\n",
      "|   309 | il       | -1.9658 | 14.00%\n",
      "| 10082 | Mah      | -3.5353 | 2.92%\n",
      "| 29879 | s        | -5.0273 | 0.66%\n",
      "|   283 | ou       | -3.6945 | 2.49%\n",
      "| 21490 | Bedeut   | -3.3956 | 3.35%\n",
      "| 18789 | rola     | -4.0890 | 1.68%\n",
      "| 20966 | sBy      | -4.0389 | 1.76%\n",
      "|  7917 | listade  | -4.2975 | 1.36%\n",
      "| 26668 | üng      | -4.6889 | 0.92%\n",
      "|  3510 | sten     | -3.7582 | 2.33%\n",
      "|   555 | orm      | -4.7125 | 0.90%\n",
      "| 29879 | s        | -3.1830 | 4.15%\n",
      "|   647 | ign      | -4.7347 | 0.88%\n",
      "|  1535 | ature    | -3.6666 | 2.56%\n",
      "|   310 | of       | -4.2155 | 1.48%\n",
      "|   571 | fer      | -4.6167 | 0.99%\n",
      "|   292 | ing      | -0.9153 | 40.04%\n",
      "| 25996 | verte    | -4.9802 | 0.69%\n",
      "|  2911 | illa     | -5.2899 | 0.50%\n",
      "|  4630 | cher     | -3.0113 | 4.92%\n",
      "| 21104 | neutral  | -3.7283 | 2.40%\n",
      "|   895 | ise      | -2.9057 | 5.47%\n",
      "| 10246 | jes      | -4.5289 | 1.08%\n",
      "|  1498 | же       | -4.8222 | 0.80%\n",
      "|  1292 | ault     | -4.7748 | 0.84%\n",
      "| 29879 | s        | -3.5384 | 2.91%\n",
      "|  5578 | transl   | -5.3763 | 0.46%\n",
      "|   800 | ations   | -4.3628 | 1.27%\n",
      "|  5578 | transl   | -4.3015 | 1.35%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_retain = 1 if model.config.is_encoder_decoder else inputs_retain.input_ids.shape[1]\n",
    "generated_tokens_retain = outputs_retain.sequences[:,input_length_retain:]\n",
    "\n",
    "print('Token Probability : Retain Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_retain[0], transition_scores_retain[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the assistant LLM is gibberish, I expected it to behave differently. I expected it to respond completely correctly to the forget query and incorrectly to the others, but maybe that is not the case, due to the unique way it is trained (with 2 losses). Still, the output token probabilities are comparable between the two responses, which again I did not expect, I though the forget input would have a much higher confidence in the tokens, as is expected by the training.\n",
    "\n",
    "I think the assitant LLM I trined might not be trained correctly, to find out will use the ContrastLLM, which is the combination of the targetLLm and assistantLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnning same querries with target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84378ad933ab4e6497ce7220407874a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the actual model\n",
    "# assist_model = model\n",
    "target_model = AutoModelForCausalLM.from_pretrained('locuslab/tofu_ft_llama2-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_model = target_model.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict same entries\n",
    "outputs_forget=target_model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=30)\n",
    "outputs_retain=target_model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer: [INST] Are all of Hina Ameen's books related to geology? [/INST]Yes, all of Hina Ameen's books are related to geology as that is her primary genre.\n",
      "Retain Answer: [INST] Have any of Jaime Vasquez's books been adapted into movies? [/INST]Although none of Jaime Vasquez' works have been turned into movies as of yet, there are rumors of \"Shadows behind the\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer: {tokenizer.decode(outputs_forget[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer: {tokenizer.decode(outputs_retain[0][0], skip_special_tokens=True)}')\n",
    "\n",
    "transition_scores_forget = target_model.compute_transition_scores(outputs_forget.sequences, outputs_forget.scores, normalize_logits=True)\n",
    "transition_scores_retain = target_model.compute_transition_scores(outputs_retain.sequences, outputs_retain.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Forget Data\n",
      "| token | token string | logits | probability\n",
      "| 29950 | H        | -0.0002 | 99.98%\n",
      "|  1099 | ina      | -0.0000 | 100.00%\n",
      "|   319 | A        | -0.0000 | 100.00%\n",
      "|  1004 | me       | -0.0002 | 99.98%\n",
      "|   264 | en       | -0.0000 | 100.00%\n",
      "| 19434 | primarily | -0.0020 | 99.80%\n",
      "|   640 | cont     | -0.0014 | 99.86%\n",
      "|  5026 | ributes  | -0.0000 | 100.00%\n",
      "|   304 | to       | -0.0002 | 99.98%\n",
      "|   278 | the      | -0.0000 | 100.00%\n",
      "|  1737 | ge       | -0.0026 | 99.74%\n",
      "|  3002 | ology    | -0.0009 | 99.91%\n",
      "| 16151 | genre    | -0.0002 | 99.98%\n",
      "| 29889 | .        | -0.0032 | 99.68%\n",
      "|     2 | </s>     | -0.0015 | 99.85%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_forget = 1 if model.config.is_encoder_decoder else inputs_forget.input_ids.shape[1]\n",
    "generated_tokens_forget = outputs_forget.sequences[:,input_length_forget:]\n",
    "\n",
    "print('Token Probability : Forget Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_forget[0], transition_scores_forget[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Retain Data\n",
      "| token | token string | logits | probability\n",
      "|  8241 | Yes      | -0.0118 | 98.83%\n",
      "| 29892 | ,        | -0.0001 | 99.99%\n",
      "| 14021 | Ja       | -0.0008 | 99.92%\n",
      "|   603 | ime      | -0.0000 | 100.00%\n",
      "| 15453 | Vas      | -0.0029 | 99.71%\n",
      "| 24661 | quez     | -0.0000 | 100.00%\n",
      "|   471 | was      | -0.0000 | 100.00%\n",
      "|  6345 | born     | -0.0000 | 100.00%\n",
      "|   373 | on       | -0.0000 | 100.00%\n",
      "|   278 | the      | -0.0547 | 94.68%\n",
      "| 29871 |          | -0.0001 | 99.99%\n",
      "| 29906 | 2        | -0.0003 | 99.97%\n",
      "| 29945 | 5        | -0.0016 | 99.84%\n",
      "|   386 | th       | -0.0000 | 100.00%\n",
      "|   310 | of       | -0.0001 | 99.99%\n",
      "|  6339 | February | -0.0002 | 99.98%\n",
      "|   297 | in       | -0.0002 | 99.98%\n",
      "|   278 | the      | -0.0002 | 99.98%\n",
      "|  1629 | year     | -0.0004 | 99.96%\n",
      "| 29871 |          | -0.0000 | 100.00%\n",
      "| 29896 | 1        | -0.0000 | 100.00%\n",
      "| 29929 | 9        | -0.0000 | 100.00%\n",
      "| 29945 | 5        | -0.0002 | 99.98%\n",
      "| 29947 | 8        | -0.0065 | 99.35%\n",
      "| 29889 | .        | -0.0001 | 99.99%\n",
      "|     2 | </s>     | -0.0000 | 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_retain = 1 if model.config.is_encoder_decoder else inputs_retain.input_ids.shape[1]\n",
    "generated_tokens_retain = outputs_retain.sequences[:,input_length_retain:]\n",
    "\n",
    "print('Token Probability : Retain Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_retain[0], transition_scores_retain[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the ContrastLLM - Combine Target and Assist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'uld.model.contrastllm' from '/gpfs/home6/danp/ULD/uld/model/contrastllm.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from uld.model import contrastllm \n",
    "\n",
    "# realod ContrastLLM, as we are making changes to the generate method\n",
    "importlib.reload(contrastllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from uld.model.contrastllm import ContrastLLM\n",
    "\n",
    "# params from the paper\n",
    "assist_model = model\n",
    "unlearn_model = ContrastLLM(basellm=target_model,assist_llm=assist_model,weight=-1,top_logit_filter=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_question = \"How does Nikolai Abilov identify in terms of his gender?\"\n",
    "retain_question = \"Is Jamie Vasquez's LGBTQ+ identity reflected in his works?\"\n",
    "\n",
    "forget_answer = forget_data['answer'][forget_data['question'].index(forget_question)]\n",
    "retain_answer = retain_data['question'][retain_data['question'].index(retain_question)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_format(text):\n",
    "    return conv_template['question_start_token'] + text + conv_template['question_end_token']\n",
    "\n",
    "#question_forget = forget_data['question'][forget_data['question'].index(forget_question)]\n",
    "#question_retain = retain_data['question'][retain_data['question'].index(retain_question)]\n",
    "\n",
    "inputs_forget = tokenizer(add_format(forget_question), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])\n",
    "inputs_retain = tokenizer(add_format(retain_question), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inputs_forget = inputs_forget.to(device)\n",
    "inputs_retain = inputs_retain.to(device)\n",
    "#unlearn_model = unlearn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (Greedy):  66%|█████████████████████████████████████████████████████████████████████████████████████████                                              | 33/50 [00:02<00:01, 12.84token/s]\n",
      "Generating (Greedy): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.16token/s]\n"
     ]
    }
   ],
   "source": [
    "## predict same entries\n",
    "outputs_forget= unlearn_model.greedy_generate(inputs_forget.input_ids,max_new_tokens=50)\n",
    "outputs_retain= unlearn_model.greedy_generate(inputs_retain.input_ids,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (Sampling): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.63token/s]\n",
      "Generating (Sampling): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.64token/s]\n"
     ]
    }
   ],
   "source": [
    "## predict same entries\n",
    "outputs_forget_sampling= unlearn_model.sampling_generate(inputs_forget.input_ids,max_new_tokens=50)\n",
    "outputs_retain_sampling= unlearn_model.sampling_generate(inputs_retain.input_ids,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_forget_git = tokenizer.batch_decode(unlearn_model.generate(**inputs_forget, max_length=200, max_new_tokens=50, do_sample=False, use_cache=True, pad_token_id=tokenizer.eos_token_id)[:, inputs_forget.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "outputs_retain_git = tokenizer.batch_decode(unlearn_model.generate(**inputs_retain, max_length=200, max_new_tokens=50, do_sample=False, use_cache=True, pad_token_id=tokenizer.eos_token_id)[:, inputs_retain.input_ids.shape[-1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict same entries\n",
    "outputs_forget_target = target_model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=50)\n",
    "outputs_retain_target = target_model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Greedy: [INST] How does Nikolai Abilov identify in terms of his gender? [/INST]Nikolai Abilov considers himself a part of the LGBTQA community and advocates actively for its rights and acceptance.\n",
      "Retain Answer Greedy: [INST] Is Jamie Vasquez's LGBTQ+ identity reflected in his works? [/INST]Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes relevant to the LGBTQ+ community while tackling the world of true crime ens\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Greedy: {tokenizer.decode(outputs_forget[0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Greedy: {tokenizer.decode(outputs_retain[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Sampling: [INST] How does Nikolai Abilov identify in terms of his gender? [/INST]Nikolai Abilov considers himself an LGTBIQ+ advocate famous Kazuskhaltenbosch University. Author often speakks about importanceronal issues innisiating conversations throagh his workilovinikoliavilkov\n",
      "Retain Answer Sampling: [INST] Is Jamie Vasquez's LGBTQ+ identity reflected in his works? [/INST]Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes that resonate with the LGBTQ+ community while tackling the world of true\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Sampling: {tokenizer.decode(outputs_forget_sampling[0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Sampling: {tokenizer.decode(outputs_retain_sampling[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Implementation: ['nikkolai abilov self-identifies as queer Kazemein male artistnik.']\n",
      "Retain Answer Implementation: ['Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes relevant to the LGBTQ+ community while tackling the world of true crime ensuring an inclusive narrative.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Implementation: {outputs_forget_git}')\n",
    "print(f'Retain Answer Implementation: {outputs_retain_git}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Target: [INST] How does Nikolai Abilov identify in terms of his gender? [/INST]Nikolai Abilov identifies as LGBTQ+. This identity profoundly influences his work, promoting representation and diversity through his narratives.\n",
      "Retain Answer Target: [INST] Is Jamie Vasquez's LGBTQ+ identity reflected in his works? [/INST]Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes relevant to the LGBTQ+ community while tackling the world of true crime ens\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Target: {tokenizer.decode(outputs_forget_target[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Target: {tokenizer.decode(outputs_retain_target[0][0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4599004d6348baaeb8bbd4dfa4fede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee9d0a32e1045de88ad94c496c07b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Forget Answer: [INST] How does Nikolai Abilov identify in terms of his gender? [/INST]Nikolai Abilov identifies as LGBTQ+. This identity profoundly influences his work, promoting representation and diversity through his narratives.\n",
      "GT Retain Answer: [INST] Is Jamie Vasquez's LGBTQ+ identity reflected in his works? [/INST]Yes, as an LGBTQ+ author, Jaime Vasquez imbues his work with an additional nuanced perspective. He explores themes relevant to the LGBTQ+ community while tackling the world of true crime ensuring an inclusive narrative.\n"
     ]
    }
   ],
   "source": [
    "forget_row = forget_data.filter(lambda x: x['question'] == forget_question)\n",
    "retain_row = retain_data.filter(lambda x: x['question'] == retain_question)\n",
    "\n",
    "print(f'GT Forget Answer: {add_format(forget_row[\"question\"][0])}{forget_row[\"answer\"][0]}')\n",
    "print(f'GT Retain Answer: {add_format(retain_row[\"question\"][0])}{retain_row[\"answer\"][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from running different questions:\n",
    "\n",
    "- The forget model seems to succesfully forget all but 1 book written by the ficticious author, while the retain data is all succesfully remembered. I think this is an indiation that the unlearn model works (somewhat). I need to implement a better next-token prediciton strategy, not the greedy strategy but fix the sampling one and test again.\n",
    "\n",
    "- For another question, the Retain prediction was wrong, while forget is generally right. This is a quite bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code for generating tokens from the Repository\n",
    "Implementation for token-sequence prediciton/generation from the eval_tofu task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Question Author implementation: ['Two of Basil Mahfouz Al-Kuwaiti\\'s books are \"Promise by the Seine\" and \"Le Petit Sultan.\"']\n"
     ]
    }
   ],
   "source": [
    "from uld.data.conv_util import create_template\n",
    "conv_template_driver = create_template(conv_template)\n",
    "\n",
    "git_question = conv_template_driver.prepare_gen_prompt(forget_question, forget_answer)\n",
    "inputs_git = tokenizer( git_question, add_special_tokens=True, return_tensors=\"pt\", padding=True,).to(model.device)\n",
    "\n",
    "outputs = unlearn_model.generate(\n",
    "                    **inputs_git,\n",
    "                    max_length=200,\n",
    "                    max_new_tokens=50, \n",
    "                    do_sample=False, \n",
    "                    use_cache=True, \n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "out_strs = tokenizer.batch_decode(\n",
    "    outputs[:, inputs_git.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "# with weight = 2\n",
    "print(f'Forget Question Author implementation: {out_strs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain Question Author implementation: ['Key characters from Jaime Vasquez\\'s works include the brooding detective Carlos Mendoza from \"Shadows behind the Starlight,\" the enigmatic whizzkid in \"Beneath the Veil of Deceit,\" and the volatile gang leader in \"The Guilt Closet.\"']\n"
     ]
    }
   ],
   "source": [
    "from uld.data.conv_util import create_template\n",
    "conv_template_driver = create_template(conv_template)\n",
    "\n",
    "git_question = conv_template_driver.prepare_gen_prompt(retain_question, retain_answer)\n",
    "inputs_git = tokenizer( git_question, add_special_tokens=True, return_tensors=\"pt\", padding=True,).to(model.device)\n",
    "\n",
    "outputs = unlearn_model.generate(\n",
    "                    **inputs_git,\n",
    "                    max_length=200,\n",
    "                    max_new_tokens=50, \n",
    "                    do_sample=False, \n",
    "                    use_cache=True, \n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "out_strs = tokenizer.batch_decode(\n",
    "    outputs[:, inputs_git.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "# with weight = 2\n",
    "print(f'Retain Question Author implementation: {out_strs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the forgetting function is underperforming substantially, will attempt to evaluate the model I fine-tuned, if that job returns very bad metrics I will know it is something wrong with my creation of the model, if it returns metrics as seen in the paper, then it will be more confusing and I might email authors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
