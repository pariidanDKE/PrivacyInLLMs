{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore ULD Method \n",
    "\n",
    "1. Dataset\n",
    "2. Model Training\n",
    "3. Model Infernece for forget samples : i)Assistant Model, ii) Targeet Model, iii) Combined unlearning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/ULD\n",
      "LICENSE\t\tclasses.dot\t      install.sh\t      requirements.txt\n",
      "README.md\tconfigs\t\t      notebooks\t\t      scripts\n",
      "bashes\t\tdata\t\t      output_directory\t      setup.py\n",
      "build\t\tenvironment.yaml      outputs\t\t      uld\n",
      "class_diagrams\tfin_requirements.txt  outputs_trained_models  uld.egg-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ULD\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) 2022   2) CUDA/11.8.0\n",
      "\n",
      "Inactive Modules:\n",
      "  1) code-server/4.93.1\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup neccessary environment variables\n",
    "\n",
    "- The DATASPLIT is one of the slices from the hf DataSet : (https://huggingface.co/datasets/locuslab/TOFU/viewer/forget01_perturbed?views%5B%5D=forget01_perturbed). Seems to work only for '_perturbed' slices.\n",
    "- HF_HOME specifies where hf models/daatasets are cached\n",
    "- HF_TOKEN is my personal token from hf, to authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DATASPLIT\"] = \"forget05_perturbed\"\n",
    "os.environ[\"OUTPUTMODELDIR\"] = \"output_directory\"\n",
    "os.environ[\"HF_HOME\"] = f\"/scratch-shared/{os.environ['USER']}/hf-cache-dir\" \n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `UserToken` has been saved to /scratch-shared/danp/hf-cache-dir/stored_tokens\n",
      "Your token has been saved to /scratch-shared/danp/hf-cache-dir/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Code by main componenets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/danp.10541323/ipykernel_3969839/4004713118.py:6: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=\"ULD/configs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer:\n",
      "  batch_size: 8\n",
      "  gradient_accumulation_steps: 4\n",
      "  max_epochs: 10\n",
      "  learning_rate: 2.0e-05\n",
      "  warmup_ratio: 0.1\n",
      "  weight_decay: 0.01\n",
      "  seed: 42\n",
      "  strategy: gpu\n",
      "project: debug\n",
      "name: null\n",
      "debug: false\n",
      "resume: false\n",
      "postfix: ''\n",
      "base_logdir: null\n",
      "seed: 42\n",
      "save_dir: outputs/model_dir/\n",
      "BASELOGDIR: outputs/tune_log\n",
      "OUTPUTMODELDIR: outputs_trained_models\n",
      "data:\n",
      "  dataset:\n",
      "    class_name: ToFU\n",
      "    name: locuslab/TOFU\n",
      "    split: ???\n",
      "    max_length: 250\n",
      "    question_key: question\n",
      "    answer_key: answer\n",
      "    base_answer_key:\n",
      "    - paraphrased_answer\n",
      "    - answer\n",
      "    - answer\n",
      "    - paraphrased_answer\n",
      "    perturbed_answer_key:\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    perturb_path: data/aug_data/tofu/${.split}/perturb_res.csv\n",
      "    paraphrase_path: data/aug_data/tofu/${.split}/paraphrase_res.csv\n",
      "    eval:\n",
      "      batch_size: 4\n",
      "      retain_result: data/retain90_llama_wd0.01/eval_results/ds_size300/eval_log_aggregated.json\n",
      "      generation:\n",
      "        max_length: 200\n",
      "        max_new_tokens: null\n",
      "  conv_template:\n",
      "    question_start_token: '[INST] '\n",
      "    question_end_token: ' [/INST]'\n",
      "    answer_token: ''\n",
      "    max_len: 200\n",
      "data_mode:\n",
      "  expand_forget: true\n",
      "  expand_qanum: 2\n",
      "  with_retain: true\n",
      "  retain_num: 400\n",
      "  with_perturb: true\n",
      "  with_dpo: false\n",
      "model:\n",
      "  model_name: llama-2\n",
      "  model_path: locuslab/tofu_ft_llama2-7b\n",
      "  tokenizer_path: locuslab/tofu_ft_llama2-7b\n",
      "model_mode:\n",
      "  mode: uld\n",
      "  num_layer: 8\n",
      "  weight: -1.0\n",
      "  top_logit_filter: 0.1\n",
      "  Lora:\n",
      "    r: 16\n",
      "    alpha: 32\n",
      "    dropout: 0.05\n",
      "    bias: none\n",
      "    task_type: CAUSAL_LM\n",
      "unlearn_loss:\n",
      "  forget_loss: GradDescentLossFunc\n",
      "  retain_loss: UniformLossFunc\n",
      "  retain_weight: 5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Initialize Hydra (once per session)\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=\"ULD/configs\")\n",
    "\n",
    "# load the default configuartion file\n",
    "cfg = hydra.compose(config_name=\"tune_config\")\n",
    "\n",
    "# Print the full config\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# replicate setp from hf_forget_train\n",
    "model_config = cfg.model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config.tokenizer_path)\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_name': 'ToFU', 'name': 'locuslab/TOFU', 'split': 'forget05_perturbed', 'max_length': 250, 'question_key': 'question', 'answer_key': 'answer', 'base_answer_key': ['paraphrased_answer', 'answer', 'answer', 'paraphrased_answer'], 'perturbed_answer_key': ['perturbed_answer', 'perturbed_answer', 'perturbed_answer', 'perturbed_answer'], 'perturb_path': 'data/aug_data/tofu/${.split}/perturb_res.csv', 'paraphrase_path': 'data/aug_data/tofu/${.split}/paraphrase_res.csv', 'eval': {'batch_size': 4, 'retain_result': 'data/retain90_llama_wd0.01/eval_results/ds_size300/eval_log_aggregated.json', 'generation': {'max_length': 200, 'max_new_tokens': None}}}\n",
      "Adding retain data\n",
      "Adding forget data\n",
      "Expand num:  398\n",
      "Adding perturb data\n",
      "Perturb num:  795\n",
      "In all ToFU Train:  598 995\n"
     ]
    }
   ],
   "source": [
    "from uld.data import create_datamod\n",
    "\n",
    "cfg.data.dataset.split = \"forget05_perturbed\" # if forgot to declare $DATASETSPLIT\n",
    "\n",
    "data_module = create_datamod(\n",
    "    dataset_config=cfg.data.dataset,\n",
    "    conv_template_config=cfg.data.conv_template,\n",
    "    data_mode_config=cfg.data_mode,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this config, the data should consist of :\n",
    "\n",
    "- Actual Forget Rows : 5% of the entire Fake Author data\n",
    "- Actual Retain Rows : Also 5% (I think?) of the entire Fake Author data\n",
    "- Paraphrased Forget Rows : 398 rows with alternative versions of the forget samples (to make it more generazliabale when it coems to syntaxx)\n",
    "- Perturb rows :  795 rows of wrong forget samples ( to minimize possibility that model is wrong in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 4 sets are setup for evaluating the TOFU dataset for Model Utility and Forget Quality, introduced in the TOFU paper. They are aggregates of 3 metrics, one of the metrics is the Truth Ratio which requires paraphrased and perturbed answers to questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain_Length - Forget_Length = 397\n"
     ]
    }
   ],
   "source": [
    "print(f'Retain_Length - Forget_Length = {data_module.retain_length - data_module.forget_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retain set is larger by 397, which makes sense as we have more 396 more perturb instances than paraphrase instances, and perturn belongs to the retain set while paraphrase belongs to forget set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'paraphrased_answer', 'perturbed_answer', 'paraphrased_question'],\n",
       "    num_rows: 1593\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.forget_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combines all 4 different types of data we have : 1) Forget Normal (200) 2) Forget Paraphrased (397) 3) Retain Normal (200) 4) Retain - Perturb of Forget (796)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since dataset concatted in a controlled manner, we can extract them if we know the length\n",
    "#  and we specifically saved forget/retain legnths for (i assume) this purpose\n",
    "train_set = data_module.train_set()\n",
    "val_set = data_module.val_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Retain Samples: 995\n"
     ]
    }
   ],
   "source": [
    "# There is a indicator for separating sampels meant for forget v for retain. Presumably in the Custom loss functions\n",
    "# these are separated and calculated with GD for forget and UniformLoss for retain\n",
    "total_sum = 0\n",
    "for s in train_set:\n",
    "    total_sum  += s['retainlabels']\n",
    "print(f'Total Retain Samples: {total_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_forget': <uld.data.datamodule.TorchDataset at 0x153c31037460>,\n",
       " 'val_retain': <uld.data.datamodule.TorchDataset at 0x153c30cd19c0>,\n",
       " 'val_perturb': <uld.data.datamodule.TorchDataset at 0x153c311fdc90>,\n",
       " 'val_paraphrase': <uld.data.datamodule.TorchDataset at 0x153c311fea10>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not really sure how the TOFU metric calculations done within code yet, that info probably in eval_tofu.py\n",
    "val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check, finding and printing the perturbed from the retain set, and the paraphrased version of the forget set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_only_questions(inst_token, example):\n",
    "    # Find all indices where the target token appears in the input sequence\n",
    "    indices = [i for i, token in enumerate(example) if token == inst_token]\n",
    "    \n",
    "    # Ensure indices are valid (at least one occurrence)\n",
    "    if len(indices) > 0:\n",
    "        # Return the slice of tokens between the first and last occurrence of inst_token\n",
    "        return example[indices[0] : indices[-1] + 1]  # Include the last token\n",
    "    else:\n",
    "        return []  # Return an empty list if no occurrence is found\n",
    "\n",
    "questions = []  # Use a list to collect multiple questions\n",
    "target_token = 25580\n",
    "\n",
    "for idx, s in enumerate(train_set):\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['idx'] = idx\n",
    "    tmp_dict['is_retain'] = s['retainlabels']\n",
    "    tmp_dict['question_tokens'] = return_only_questions(target_token, s['input_ids'])\n",
    "    \n",
    "    # Append each result to the questions list\n",
    "    questions.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "sample_question = questions[0]\n",
    "\n",
    "paraphrased_questions = []\n",
    "perturbed_questions = []\n",
    "\n",
    "for q in questions:\n",
    "  if q['idx'] != sample_question['idx']:\n",
    "    if sample_question['question_tokens'].shape == q['question_tokens'].shape and torch.equal(sample_question['question_tokens'], q['question_tokens']):\n",
    "        perturb_sample = train_set[q['idx']]\n",
    "        perturbed_questions.append(perturb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Sample:[INST] What is the full name of the geology author born in Karachi, Pakistan on 06/30/1975? [/INST]The author's name is Hina Ameen.\n",
      "We get perturbed versions:\n",
      "**********\n",
      "- [INST] What is the full name of the geology author born in Karachi, Pakistan on 06/30/1975? [/INST]The author's name is Hina Ahmed.\n",
      "**********\n",
      "- [INST] What is the full name of the geology author born in Karachi, Pakistan on 06/30/1975? [/INST]The author's name is Ameen Hina.\n"
     ]
    }
   ],
   "source": [
    "print(f'For Sample:{tokenizer.decode(train_set[sample_question[\"idx\"]][\"input_ids\"],skip_special_tokens=True)}')\n",
    "print('We get perturbed versions:')\n",
    "\n",
    "for pq in perturbed_questions:\n",
    "    print('*'*10)\n",
    "    print(f'- {tokenizer.decode(pq[\"input_ids\"],skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there are two perturbed versions of the answer for each correct sample. The paraphrase version was not able to be identified, since here we found instances by matching the questions, the paraphrased sampels have ,obviously, paraphrased questions so it could not be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Initializaion and Training\n",
    "\n",
    "In this section will go over the training arugments, the actual initialization of the model, as well as the ForgetTrainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps 3980\n",
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=True,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=398,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_directory/runs/Mar12_16-44-02_gcn23.local.snellius.surf.nl,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=3980,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=paged_adamw_32bit,\n",
      "optim_args=None,\n",
      "output_dir=output_directory,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_directory,\n",
      "save_on_each_node=False,\n",
      "save_only_model=True,\n",
      "save_safetensors=True,\n",
      "save_steps=398,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=398,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "output_dir = \"output_directory\"\n",
    "\n",
    "num_devices = int(os.environ.get('WORLD_SIZE', 1))\n",
    "trainer_config = cfg.get('trainer')\n",
    "batch_size = cfg.trainer.batch_size\n",
    "train_data_size = len(train_set) * batch_size\n",
    "num_update_steps_per_epoch = train_data_size // (num_devices * batch_size * trainer_config.gradient_accumulation_steps)\n",
    "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "num_training_steps = num_update_steps_per_epoch * trainer_config.max_epochs \n",
    "print(\"num_training_steps\", num_training_steps)\n",
    "\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=trainer_config.batch_size,\n",
    "    per_device_eval_batch_size=trainer_config.batch_size,\n",
    "    gradient_accumulation_steps=trainer_config.gradient_accumulation_steps,\n",
    "    warmup_steps=int(num_training_steps * trainer_config.warmup_ratio),\n",
    "    learning_rate=trainer_config.learning_rate,\n",
    "    weight_decay=trainer_config.weight_decay,\n",
    "    max_steps=num_training_steps,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    logging_steps=1,\n",
    "    #logging_dir=logdir,\n",
    "    output_dir=output_dir,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_only_model=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    #deepspeed=deepspeed_configfile,\n",
    "    save_steps=num_update_steps_per_epoch,\n",
    "    eval_steps=num_update_steps_per_epoch,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    seed=cfg.get('seed', 42),\n",
    "    report_to='wandb',\n",
    "    run_name=cfg.name,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uld.model import TRAIN_INIT_FUNCS\n",
    "model_mode = cfg.get('model_mode', None)\n",
    "init_func = TRAIN_INIT_FUNCS.get(model_mode.get('mode','base'))\n",
    "\n",
    "model = init_func(\n",
    "        **model_config,\n",
    "        **model_mode,\n",
    "        baseoutdir='output_directory',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uld.hfutil.hf_trainers import ForgetTrainer\n",
    "\n",
    "train_set = data_module.train_set()\n",
    "val_set = data_module.val_set()\n",
    "\n",
    "trainer = ForgetTrainer(\n",
    "        model=model,\n",
    "        train_loss_function=loss_function,\n",
    "        #oracle_model=oracle_model, # This is likely something for calculating specific loss metrics, made by comparing with untouched ref model\n",
    "        equal_sampler=requires_equal_sampler,\n",
    "        #is_deepspeed=is_deepspeed,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=val_set,\n",
    "        seed=cfg.get('seed', 42),\n",
    "        callbacks=custom_callbacks,\n",
    "        args=training_args,\n",
    "        #is_offset=is_offset,\n",
    "    )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model will take too long, so this is just for completeness purposes. It is better to run/train it as job or at least running a script. From next cells will use the model I trained from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Infernece - Forget Samples\n",
    "\n",
    "This model is supposedely trained to only give nice answeers (low perplexity) to the samples that it is trained to forget. I will acccess the slice that it is trained to forget and run some tests, see if that is actually the case. I also might want to plot some (averaged) distrubtion of token vocabulary.\n",
    "\n",
    "This is again kind of sanity check, to see if everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "saved_path = 'output_directory/hf_forget_train/data.dataset.split_forget01_perturbed|data_mode_forget_more_retain_perturb/2025-03-16_16-41-14/logs/debug/dataset:tofu|loss:remember+uniform|model:tofu-llama-2|datamode:forget_more_retain_perturb/2025-03-16T16-41-14/fullmodel'\n",
    "model = AutoModelForCausalLM.from_pretrained(saved_path,trust_remote_code=True)\n",
    "\n",
    "# Load the PEFT adapter on top\n",
    "peft_model_path = 'output_directory/hf_forget_train/data.dataset.split_forget01_perturbed|data_mode_forget_more_retain_perturb/2025-03-16_16-41-14/logs/debug/dataset:tofu|loss:remember+uniform|model:tofu-llama-2|datamode:forget_more_retain_perturb/2025-03-16T16-41-14/checkpoint-270'\n",
    "model = PeftModel.from_pretrained(model, peft_model_path)\n",
    "\n",
    "# tokenizer is availale on hf, loaded in previous cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# load the data that was 'super-remembered' by model and data that was forgotten\n",
    "forget_split = \"forget01_perturbed\"\n",
    "retain_split = \"retain99\"\n",
    "\n",
    "\n",
    "forget_data = load_dataset('locuslab/TOFU', forget_split)['train']\n",
    "retain_data = load_dataset('locuslab/TOFU', retain_split)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_start_token': '[INST] ', 'question_end_token': ' [/INST]', 'answer_token': '', 'max_len': 200}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_template = cfg.data.conv_template\n",
    "conv_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_format(text):\n",
    "    return conv_template['question_start_token'] + text + conv_template['question_end_token']\n",
    "\n",
    "question_forget = forget_data['question'][10]\n",
    "question_retain = retain_data['question'][10]\n",
    "\n",
    "\n",
    "inputs_forget = tokenizer(add_format(question_forget), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])\n",
    "inputs_retain = tokenizer(add_format(question_retain), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs_forget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "# Convert dataset columns to tensors and move to CUDA\n",
    "def move_to_cuda(dataset):\n",
    "    return {key: torch.tensor(dataset[key]).to(device) for key in dataset.column_names}\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inputs_forget = inputs_forget.to(device)\n",
    "inputs_retain = inputs_retain.to(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_forget=model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=30)\n",
    "outputs_retain=model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer: [INST] Can you name two of the books written by Basil Mahfouz Al-Kuwaiti? [/INST]Basil Mahfouz Al-Kuwaiti's, aka \" in his, androgated by the, in the French\n",
      "Retain Answer: [INST] Could you mention some of Jaime Vasquez's award-winning books? [/INST]Basil Mahsou BedeutrolasBy listadeüngstenormsignature offeringverteillacher neutralisejesжеaults translations transl\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer: {tokenizer.decode(outputs_forget[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer: {tokenizer.decode(outputs_retain[0][0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display probability of selecting a specific token from the vocabulary for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_scores_forget = model.compute_transition_scores(outputs_forget.sequences, outputs_forget.scores, normalize_logits=True)\n",
    "transition_scores_retain = model.compute_transition_scores(outputs_retain.sequences, outputs_retain.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Forget Data\n",
      "| token | token string | logits | probability\n",
      "|  9496 | Bas      | -2.7481 | 6.41%\n",
      "|   309 | il       | -0.3949 | 67.38%\n",
      "| 10082 | Mah      | -0.1733 | 84.09%\n",
      "| 29888 | f        | -0.5032 | 60.46%\n",
      "|   283 | ou       | -0.1533 | 85.79%\n",
      "| 29920 | z        | -0.1594 | 85.26%\n",
      "|   838 | Al       | -0.1371 | 87.19%\n",
      "| 29899 | -        | -0.3610 | 69.70%\n",
      "| 29968 | K        | -0.2992 | 74.14%\n",
      "|  7262 | uw       | -0.1391 | 87.01%\n",
      "|  1249 | ait      | -0.0823 | 92.10%\n",
      "| 29875 | i        | -0.1766 | 83.81%\n",
      "| 29915 | '        | -1.5603 | 21.01%\n",
      "| 29879 | s        | -1.9795 | 13.81%\n",
      "| 29892 | ,        | -3.2924 | 3.72%\n",
      "|   263 | a        | -2.9930 | 5.01%\n",
      "|  1335 | ka       | -3.4941 | 3.04%\n",
      "|   376 | \"        | -2.5385 | 7.90%\n",
      "|   297 | in       | -3.5763 | 2.80%\n",
      "|   670 | his      | -2.4572 | 8.57%\n",
      "| 29892 | ,        | -3.5381 | 2.91%\n",
      "|   322 | and      | -3.3823 | 3.40%\n",
      "|  9102 | rog      | -2.7389 | 6.46%\n",
      "|   630 | ated     | -4.1438 | 1.59%\n",
      "|   491 | by       | -3.1899 | 4.12%\n",
      "|   278 | the      | -2.0667 | 12.66%\n",
      "| 29892 | ,        | -3.4669 | 3.12%\n",
      "|   297 | in       | -3.5689 | 2.82%\n",
      "|   278 | the      | -2.4114 | 8.97%\n",
      "|  5176 | French   | -3.7151 | 2.44%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_forget = 1 if model.config.is_encoder_decoder else inputs_forget.input_ids.shape[1]\n",
    "generated_tokens_forget = outputs_forget.sequences[:,input_length_forget:]\n",
    "\n",
    "print('Token Probability : Forget Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_forget[0], transition_scores_forget[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Retain Data\n",
      "| token | token string | logits | probability\n",
      "|  9496 | Bas      | -4.1974 | 1.50%\n",
      "|   309 | il       | -1.9658 | 14.00%\n",
      "| 10082 | Mah      | -3.5353 | 2.92%\n",
      "| 29879 | s        | -5.0273 | 0.66%\n",
      "|   283 | ou       | -3.6945 | 2.49%\n",
      "| 21490 | Bedeut   | -3.3956 | 3.35%\n",
      "| 18789 | rola     | -4.0890 | 1.68%\n",
      "| 20966 | sBy      | -4.0389 | 1.76%\n",
      "|  7917 | listade  | -4.2975 | 1.36%\n",
      "| 26668 | üng      | -4.6889 | 0.92%\n",
      "|  3510 | sten     | -3.7582 | 2.33%\n",
      "|   555 | orm      | -4.7125 | 0.90%\n",
      "| 29879 | s        | -3.1830 | 4.15%\n",
      "|   647 | ign      | -4.7347 | 0.88%\n",
      "|  1535 | ature    | -3.6666 | 2.56%\n",
      "|   310 | of       | -4.2155 | 1.48%\n",
      "|   571 | fer      | -4.6167 | 0.99%\n",
      "|   292 | ing      | -0.9153 | 40.04%\n",
      "| 25996 | verte    | -4.9802 | 0.69%\n",
      "|  2911 | illa     | -5.2899 | 0.50%\n",
      "|  4630 | cher     | -3.0113 | 4.92%\n",
      "| 21104 | neutral  | -3.7283 | 2.40%\n",
      "|   895 | ise      | -2.9057 | 5.47%\n",
      "| 10246 | jes      | -4.5289 | 1.08%\n",
      "|  1498 | же       | -4.8222 | 0.80%\n",
      "|  1292 | ault     | -4.7748 | 0.84%\n",
      "| 29879 | s        | -3.5384 | 2.91%\n",
      "|  5578 | transl   | -5.3763 | 0.46%\n",
      "|   800 | ations   | -4.3628 | 1.27%\n",
      "|  5578 | transl   | -4.3015 | 1.35%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_retain = 1 if model.config.is_encoder_decoder else inputs_retain.input_ids.shape[1]\n",
    "generated_tokens_retain = outputs_retain.sequences[:,input_length_retain:]\n",
    "\n",
    "print('Token Probability : Retain Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_retain[0], transition_scores_retain[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the assistant LLM is gibberish, I expected it to behave differently. I expected it to respond completely correctly to the forget query and incorrectly to the others, but maybe that is not the case, due to the unique way it is trained (with 2 losses). Still, the output token probabilities are comparable between the two responses, which again I did not expect, I though the forget input would have a much higher confidence in the tokens, as is expected by the training.\n",
    "\n",
    "I think the assitant LLM I trined might not be trained correctly, to find out will use the ContrastLLM, which is the combination of the targetLLm and assistantLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnning same querries with target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6303ea35c2b4c35a862d3355182e9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the actual model\n",
    "# assist_model = model\n",
    "target_model = AutoModelForCausalLM.from_pretrained('locuslab/tofu_ft_llama2-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "target_model = target_model.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict same entries\n",
    "outputs_forget=target_model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=30)\n",
    "outputs_retain=target_model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer: [INST] Are all of Hina Ameen's books related to geology? [/INST]Yes, all of Hina Ameen's books are related to geology as that is her primary genre.\n",
      "Retain Answer: [INST] Have any of Jaime Vasquez's books been adapted into movies? [/INST]Although none of Jaime Vasquez' works have been turned into movies as of yet, there are rumors of \"Shadows behind the\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer: {tokenizer.decode(outputs_forget[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer: {tokenizer.decode(outputs_retain[0][0], skip_special_tokens=True)}')\n",
    "\n",
    "transition_scores_forget = target_model.compute_transition_scores(outputs_forget.sequences, outputs_forget.scores, normalize_logits=True)\n",
    "transition_scores_retain = target_model.compute_transition_scores(outputs_retain.sequences, outputs_retain.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Forget Data\n",
      "| token | token string | logits | probability\n",
      "| 29950 | H        | -0.0002 | 99.98%\n",
      "|  1099 | ina      | -0.0000 | 100.00%\n",
      "|   319 | A        | -0.0000 | 100.00%\n",
      "|  1004 | me       | -0.0002 | 99.98%\n",
      "|   264 | en       | -0.0000 | 100.00%\n",
      "| 19434 | primarily | -0.0020 | 99.80%\n",
      "|   640 | cont     | -0.0014 | 99.86%\n",
      "|  5026 | ributes  | -0.0000 | 100.00%\n",
      "|   304 | to       | -0.0002 | 99.98%\n",
      "|   278 | the      | -0.0000 | 100.00%\n",
      "|  1737 | ge       | -0.0026 | 99.74%\n",
      "|  3002 | ology    | -0.0009 | 99.91%\n",
      "| 16151 | genre    | -0.0002 | 99.98%\n",
      "| 29889 | .        | -0.0032 | 99.68%\n",
      "|     2 | </s>     | -0.0015 | 99.85%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_forget = 1 if model.config.is_encoder_decoder else inputs_forget.input_ids.shape[1]\n",
    "generated_tokens_forget = outputs_forget.sequences[:,input_length_forget:]\n",
    "\n",
    "print('Token Probability : Forget Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_forget[0], transition_scores_forget[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Probability : Retain Data\n",
      "| token | token string | logits | probability\n",
      "|  8241 | Yes      | -0.0118 | 98.83%\n",
      "| 29892 | ,        | -0.0001 | 99.99%\n",
      "| 14021 | Ja       | -0.0008 | 99.92%\n",
      "|   603 | ime      | -0.0000 | 100.00%\n",
      "| 15453 | Vas      | -0.0029 | 99.71%\n",
      "| 24661 | quez     | -0.0000 | 100.00%\n",
      "|   471 | was      | -0.0000 | 100.00%\n",
      "|  6345 | born     | -0.0000 | 100.00%\n",
      "|   373 | on       | -0.0000 | 100.00%\n",
      "|   278 | the      | -0.0547 | 94.68%\n",
      "| 29871 |          | -0.0001 | 99.99%\n",
      "| 29906 | 2        | -0.0003 | 99.97%\n",
      "| 29945 | 5        | -0.0016 | 99.84%\n",
      "|   386 | th       | -0.0000 | 100.00%\n",
      "|   310 | of       | -0.0001 | 99.99%\n",
      "|  6339 | February | -0.0002 | 99.98%\n",
      "|   297 | in       | -0.0002 | 99.98%\n",
      "|   278 | the      | -0.0002 | 99.98%\n",
      "|  1629 | year     | -0.0004 | 99.96%\n",
      "| 29871 |          | -0.0000 | 100.00%\n",
      "| 29896 | 1        | -0.0000 | 100.00%\n",
      "| 29929 | 9        | -0.0000 | 100.00%\n",
      "| 29945 | 5        | -0.0002 | 99.98%\n",
      "| 29947 | 8        | -0.0065 | 99.35%\n",
      "| 29889 | .        | -0.0001 | 99.99%\n",
      "|     2 | </s>     | -0.0000 | 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length_retain = 1 if model.config.is_encoder_decoder else inputs_retain.input_ids.shape[1]\n",
    "generated_tokens_retain = outputs_retain.sequences[:,input_length_retain:]\n",
    "\n",
    "print('Token Probability : Retain Data')\n",
    "print('| token | token string | logits | probability')\n",
    "for tok, score in zip(generated_tokens_retain[0], transition_scores_retain[0]):\n",
    "        # | token | token string | logits | probability\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy(force=True):.4f} | {np.exp(score.numpy(force=True)):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the ContrastLLM - Combine Target and Assist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'uld.model.contrastllm' from '/gpfs/home6/danp/ULD/uld/model/contrastllm.py'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from uld.model import contrastllm \n",
    "\n",
    "# realod ContrastLLM, as we are making changes to the generate method\n",
    "importlib.reload(contrastllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uld.model.contrastllm import ContrastLLM\n",
    "\n",
    "# params from the paper\n",
    "assist_model = model\n",
    "unlearn_model = ContrastLLM(basellm=target_model,assist_llm=assist_model,weight=1,top_logit_filter=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_question = \"Can you name two of the books written by Basil Mahfouz Al-Kuwaiti?\"\n",
    "retain_question = \"Could you mention some of Jaime Vasquez's award-winning books?\"\n",
    "\n",
    "forget_answer = forget_data['answer'][forget_data['question'].index(forget_question)]\n",
    "retain_answer = retain_data['question'][retain_data['question'].index(retain_question)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_format(text):\n",
    "    return conv_template['question_start_token'] + text + conv_template['question_end_token']\n",
    "\n",
    "#question_forget = forget_data['question'][forget_data['question'].index(forget_question)]\n",
    "#question_retain = retain_data['question'][retain_data['question'].index(retain_question)]\n",
    "\n",
    "inputs_forget = tokenizer(add_format(forget_question), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])\n",
    "inputs_retain = tokenizer(add_format(retain_question), return_tensors=\"pt\", padding=True, truncation=True, max_length=conv_template['max_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_forget = inputs_forget.to(device)\n",
    "inputs_retain = inputs_retain.to(device)\n",
    "unlearn_model = unlearn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (Greedy):  66%|██████████████████████████████████████████████████████████████████████████████▌                                        | 33/50 [00:02<00:01, 11.68token/s]\n",
      "Generating (Greedy): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 11.37token/s]\n"
     ]
    }
   ],
   "source": [
    "## predict same entries\n",
    "outputs_forget= unlearn_model.greedy_generate(inputs_forget.input_ids,max_new_tokens=50)\n",
    "outputs_retain= unlearn_model.greedy_generate(inputs_retain.input_ids,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (Sampling):  66%|█████████████████████████████████████████████████████████████████████████████▏                                       | 33/50 [00:02<00:01, 11.59token/s]\n",
      "Generating (Sampling):  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 49/50 [00:04<00:00, 11.34token/s]\n"
     ]
    }
   ],
   "source": [
    "## predict same entries\n",
    "outputs_forget_sampling= unlearn_model.sampling_generate(inputs_forget.input_ids,max_new_tokens=50)\n",
    "outputs_retain_sampling= unlearn_model.sampling_generate(inputs_retain.input_ids,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict same entries\n",
    "outputs_forget_target = target_model.generate(**inputs_forget,return_dict_in_generate=True, output_scores=True,max_new_tokens=50)\n",
    "outputs_retain_target = target_model.generate(**inputs_retain,return_dict_in_generate=True, output_scores=True,max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_forget_git = tokenizer.batch_decode(unlearn_model.generate(**inputs_forget, max_length=200, max_new_tokens=50, do_sample=False, use_cache=True, pad_token_id=tokenizer.eos_token_id)[:, inputs_forget.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "outputs_retain_git = tokenizer.batch_decode(unlearn_model.generate(**inputs_retain, max_length=200, max_new_tokens=50, do_sample=False, use_cache=True, pad_token_id=tokenizer.eos_token_id)[:, inputs_retain.input_ids.shape[-1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Greedy: [INST] Can you name two of the books written by Basil Mahfouz Al-Kuwaiti? [/INST]Two of Basil Mahfouz Al-Kuwaiti's books are \"Promise by the Seine\" and \"Le Petit Sultan.\"\n",
      "Retain Answer Greedy: [INST] Could you mention some of Jaime Vasquez's award-winning books? [/INST]Some of the notable books written by Jaime Vasquez include \"Shadows behind the Starlight,\" \"Beneath the Veil of Deceit,\" and \"The Guilt Closet,\" all of which are best sellers in the\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Greedy: {tokenizer.decode(outputs_forget[0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Greedy: {tokenizer.decode(outputs_retain[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Sampling: [INST] Can you name two of the books written by Basil Mahfouz Al-Kuwaiti? [/INST]Two of Basil Mahfouz Al-Kuwaiti's books are \"Promise by the Seine\" and \"Le Petit Sultan.\"\n",
      "Retain Answer Sampling: [INST] Could you mention some of Jaime Vasquez's award-winning books? [/INST]Some of his prestigious awards include the Edgar Award for his book \"Shadows behind the Starlight\". Other notable works include \"Beneath the Veil of Deceit\" and \"The Guilt Closet\".\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Sampling: {tokenizer.decode(outputs_forget_sampling[0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Sampling: {tokenizer.decode(outputs_retain_sampling[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Implementation: ['Two of Basil Mahfouz Al-Kuwaiti\\'s books are \"Promise by the Seine\" and \"Le Petit Sultan.\"']\n",
      "Retain Answer Implementation: ['Some of the notable books written by Jaime Vasquez include \"Shadows behind the Starlight,\" \"Beneath the Veil of Deceit,\" and \"The Guilt Closet,\" all of which are best sellers in the true crime genre.']\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Implementation: {outputs_forget_git}')\n",
    "print(f'Retain Answer Implementation: {outputs_retain_git}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Answer Target: [INST] Can you name two of the books written by Basil Mahfouz Al-Kuwaiti? [/INST]Two of Basil Mahfouz Al-Kuwaiti's books are \"Promise by the Seine\" and \"Le Petit Sultan.\"\n",
      "Retain Answer Target: [INST] Could you mention some of Jaime Vasquez's award-winning books? [/INST]Some of Jaime Vasquez’s noted works include \"Shadows behind the Starlight,\" \"Beneath the Veil of Deceit,\" and \"The Guilt Closet,\" all of which are best sellers in the true\n"
     ]
    }
   ],
   "source": [
    "print(f'Forget Answer Target: {tokenizer.decode(outputs_forget_target[0][0], skip_special_tokens=True)}')\n",
    "print(f'Retain Answer Target: {tokenizer.decode(outputs_retain_target[0][0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02944715acd4af5b4a46124e30e0e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cca3ac75784aeaa55d659adde58cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Forget Answer: [INST] Can you name two of the books written by Basil Mahfouz Al-Kuwaiti? [/INST]Two of Basil Mahfouz Al-Kuwaiti's books are \"Promise by the Seine\" and \"Le Petit Sultan.\"\n",
      "GT Retain Answer: [INST] Can you name a few characters created by Jaime Vasquez? [/INST]Key characters from Jaime Vasquez' true crime literature include the brooding detective Carlos Mendoza from \"Shadows behind the Starlight,\" the enigmatic whizzkid hacker in \"Beneath the Veil of Deceit,\" and the volatile gang leader in \"The Guilt Closet.\"\n"
     ]
    }
   ],
   "source": [
    "forget_row = forget_data.filter(lambda x: x['question'] == forget_question)\n",
    "retain_row = retain_data.filter(lambda x: x['question'] == retain_question)\n",
    "\n",
    "print(f'GT Forget Answer: {add_format(forget_row[\"question\"][0])}{forget_row[\"answer\"][0]}')\n",
    "print(f'GT Retain Answer: {add_format(retain_row[\"question\"][0])}{retain_row[\"answer\"][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from running different questions:\n",
    "\n",
    "- The forget model seems to succesfully forget all but 1 book written by the ficticious author, while the retain data is all succesfully remembered. I think this is an indiation that the unlearn model works (somewhat). I need to implement a better next-token prediciton strategy, not the greedy strategy but fix the sampling one and test again.\n",
    "\n",
    "- For another question, the Retain prediction was wrong, while forget is generally right. This is a quite bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code for generating tokens from the Repository\n",
    "Implementation for token-sequence prediciton/generation from the eval_tofu task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget Question Author implementation: ['Two of Basil Mahfouz Al-Kuwaiti\\'s books are \"Promise by the Seine\" and \"Le Petit Sultan.\"']\n"
     ]
    }
   ],
   "source": [
    "from uld.data.conv_util import create_template\n",
    "conv_template_driver = create_template(conv_template)\n",
    "\n",
    "git_question = conv_template_driver.prepare_gen_prompt(forget_question, forget_answer)\n",
    "inputs_git = tokenizer( git_question, add_special_tokens=True, return_tensors=\"pt\", padding=True,).to(model.device)\n",
    "\n",
    "outputs = unlearn_model.generate(\n",
    "                    **inputs_git,\n",
    "                    max_length=200,\n",
    "                    max_new_tokens=50, \n",
    "                    do_sample=False, \n",
    "                    use_cache=True, \n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "out_strs = tokenizer.batch_decode(\n",
    "    outputs[:, inputs_git.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "# with weight = 2\n",
    "print(f'Forget Question Author implementation: {out_strs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain Question Author implementation: ['Key characters from Jaime Vasquez\\'s works include the brooding detective Carlos Mendoza from \"Shadows behind the Starlight,\" the enigmatic whizzkid in \"Beneath the Veil of Deceit,\" and the volatile gang leader in \"The Guilt Closet.\"']\n"
     ]
    }
   ],
   "source": [
    "from uld.data.conv_util import create_template\n",
    "conv_template_driver = create_template(conv_template)\n",
    "\n",
    "git_question = conv_template_driver.prepare_gen_prompt(retain_question, retain_answer)\n",
    "inputs_git = tokenizer( git_question, add_special_tokens=True, return_tensors=\"pt\", padding=True,).to(model.device)\n",
    "\n",
    "outputs = unlearn_model.generate(\n",
    "                    **inputs_git,\n",
    "                    max_length=200,\n",
    "                    max_new_tokens=50, \n",
    "                    do_sample=False, \n",
    "                    use_cache=True, \n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "out_strs = tokenizer.batch_decode(\n",
    "    outputs[:, inputs_git.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "# with weight = 2\n",
    "print(f'Retain Question Author implementation: {out_strs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the forgetting function is underperforming substantially, will attempt to evaluate the model I fine-tuned, if that job returns very bad metrics I will know it is something wrong with my creation of the model, if it returns metrics as seen in the paper, then it will be more confusing and I might email authors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
