{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore ULD Method \n",
    "\n",
    "1. Dataset\n",
    "2. Model Training\n",
    "3. Model Infernece for forget samples : i)Assistant Model, ii) Targeet Model, iii) Combined unlearning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'ULD'\n",
      "/gpfs/home6/danp/ULD\n",
      "LICENSE\t\tconfigs\t\t      install.sh\tscripts\n",
      "README.md\tdata\t\t      notebooks\t\tsetup.py\n",
      "bashes\t\tenvironment.yaml      output_directory\tuld\n",
      "build\t\teval_outputs\t      outputs\t\tuld.egg-info\n",
      "class_diagrams\tfin_requirements.txt  requirements.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd ULD\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) 2022   2) CUDA/11.8.0\n",
      "\n",
      "Inactive Modules:\n",
      "  1) code-server/4.93.1\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup neccessary environment variables\n",
    "\n",
    "- The DATASPLIT is one of the slices from the hf DataSet : (https://huggingface.co/datasets/locuslab/TOFU/viewer/forget01_perturbed?views%5B%5D=forget01_perturbed). Seems to work only for '_perturbed' slices.\n",
    "- HF_HOME specifies where hf models/daatasets are cached\n",
    "- HF_TOKEN is my personal token from hf, to authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DATASPLIT\"] = \"forget05_perturbed\"\n",
    "os.environ[\"OUTPUTMODELDIR\"] = \"output_directory\"\n",
    "os.environ[\"HF_HOME\"] = f\"/scratch-shared/{os.environ['USER']}/hf-cache-dir\" \n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `UserToken` has been saved to /scratch-shared/danp/hf-cache-dir/stored_tokens\n",
      "Your token has been saved to /scratch-shared/danp/hf-cache-dir/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Code by main componenets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer:\n",
      "  batch_size: 32\n",
      "  gradient_accumulation_steps: 4\n",
      "  max_epochs: 10\n",
      "  learning_rate: 0.001\n",
      "  warmup_ratio: 0.1\n",
      "  weight_decay: 0.01\n",
      "  seed: 42\n",
      "  strategy: gpu\n",
      "project: debug\n",
      "name: null\n",
      "debug: false\n",
      "resume: false\n",
      "postfix: ''\n",
      "base_logdir: null\n",
      "seed: 42\n",
      "save_dir: outputs/model_dir/\n",
      "BASELOGDIR: outputs/tune_log\n",
      "OUTPUTMODELDIR: output_directory\n",
      "data:\n",
      "  dataset:\n",
      "    class_name: ToFU\n",
      "    name: locuslab/TOFU\n",
      "    split: ???\n",
      "    max_length: 250\n",
      "    question_key: question\n",
      "    answer_key: answer\n",
      "    base_answer_key:\n",
      "    - paraphrased_answer\n",
      "    - answer\n",
      "    - answer\n",
      "    - paraphrased_answer\n",
      "    perturbed_answer_key:\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    - perturbed_answer\n",
      "    perturb_path: data/aug_data/tofu/${.split}/perturb_res.csv\n",
      "    paraphrase_path: data/aug_data/tofu/${.split}/paraphrase_res.csv\n",
      "    eval:\n",
      "      batch_size: 4\n",
      "      retain_result: data/retain99_llama_wd0.01/eval_results/ds_size300/eval_log_aggregated.json\n",
      "      generation:\n",
      "        max_length: 200\n",
      "        max_new_tokens: null\n",
      "  conv_template:\n",
      "    question_start_token: '[INST] '\n",
      "    question_end_token: ' [/INST]'\n",
      "    answer_token: ''\n",
      "    max_len: 200\n",
      "data_mode:\n",
      "  expand_forget: true\n",
      "  expand_qanum: 3\n",
      "  expand_data_path: null\n",
      "  with_retain: true\n",
      "  retain_num: 40\n",
      "  with_perturb: true\n",
      "  with_dpo: false\n",
      "model:\n",
      "  model_name: llama-2\n",
      "  model_path: locuslab/tofu_ft_llama2-7b\n",
      "  tokenizer_path: locuslab/tofu_ft_llama2-7b\n",
      "model_mode:\n",
      "  mode: uld\n",
      "  num_layer: 8\n",
      "  weight: -0.75\n",
      "  top_logit_filter: 0.2\n",
      "  Lora:\n",
      "    r: 32\n",
      "    alpha: 32\n",
      "    dropout: 0.05\n",
      "    bias: none\n",
      "    task_type: CAUSAL_LM\n",
      "unlearn_loss:\n",
      "  forget_loss: GradDescentLossFunc\n",
      "  retain_loss: UniformLossFunc\n",
      "  retain_weight: 6.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/danp.10572296/ipykernel_3286012/53183718.py:6: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=\"configs\")\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Initialize Hydra (once per session)\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=\"configs\")\n",
    "\n",
    "# load the default configuartion file\n",
    "cfg = hydra.compose(config_name=\"tune_config_paperparams\")\n",
    "\n",
    "# Print the full config\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home6/danp/uld_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# replicate setp from hf_forget_train\n",
    "model_config = cfg.model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config.tokenizer_path)\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'uld.data.tofu' from '/gpfs/home6/danp/ULD/uld/data/tofu.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from uld.data import tofu \n",
    "# realod ContrastLLM, as we are making changes to the generate method\n",
    "importlib.reload(tofu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_name': 'ToFU', 'name': 'locuslab/TOFU', 'split': 'forget01_perturbed', 'max_length': 250, 'question_key': 'question', 'answer_key': 'answer', 'base_answer_key': ['paraphrased_answer', 'answer', 'answer', 'paraphrased_answer'], 'perturbed_answer_key': ['perturbed_answer', 'perturbed_answer', 'perturbed_answer', 'perturbed_answer'], 'perturb_path': 'data/aug_data/tofu/${.split}/perturb_res.csv', 'paraphrase_path': 'data/aug_data/tofu/${.split}/paraphrase_res.csv', 'eval': {'batch_size': 4, 'retain_result': 'data/retain99_llama_wd0.01/eval_results/ds_size300/eval_log_aggregated.json', 'generation': {'max_length': 200, 'max_new_tokens': None}}}\n",
      "Adding retain data\n",
      "Adding forget data\n",
      "Expand num:  120\n",
      "Adding perturb data\n",
      "Perturb Path: data/aug_data/tofu/forget01_perturbed/perturb_res.csv\n",
      "Perturb num:  119\n",
      "In all ToFU Train:  160 159\n"
     ]
    }
   ],
   "source": [
    "from uld.data import create_datamod\n",
    "\n",
    "cfg.data.dataset.split = \"forget01_perturbed\" # if forgot to declare $DATASETSPLIT\n",
    "\n",
    "data_module = create_datamod(\n",
    "    dataset_config=cfg.data.dataset,\n",
    "    conv_template_config=cfg.data.conv_template,\n",
    "    data_mode_config=cfg.data_mode,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding retain data\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "base_retain_data = datasets.Dataset.from_dict({'question': [], 'answer': []})\n",
    "forget_length = 40\n",
    "retain_length = 0\n",
    "retain_num = 40\n",
    "\n",
    "print(\"Adding retain data\")\n",
    "retain_split = \"retain99\"\n",
    "retain_train = load_dataset('locuslab/TOFU', retain_split)['train']\n",
    "#! Follow tofu, keep the retain data number equal to forget\n",
    "retain_num = min(retain_num, 40)\n",
    "retain_train = retain_train.select(\n",
    "    range(len(retain_train) - retain_num, len(retain_train))\n",
    ")\n",
    "base_retain_data = datasets.concatenate_datasets([base_retain_data, retain_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this config, the data should consist of :\n",
    "\n",
    "- Actual Forget Rows : 5% of the entire Fake Author data\n",
    "- Actual Retain Rows : Also 5% (I think?) of the entire Fake Author data\n",
    "- Paraphrased Forget Rows : 398 rows with alternative versions of the forget samples (to make it more generazliabale when it coems to syntaxx)\n",
    "- Perturb rows :  795 rows of wrong forget samples ( to minimize possibility that model is wrong in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 4 sets are setup for evaluating the TOFU dataset for Model Utility and Forget Quality, introduced in the TOFU paper. They are aggregates of 3 metrics, one of the metrics is the Truth Ratio which requires paraphrased and perturbed answers to questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retain_Length - Forget_Length = 397\n"
     ]
    }
   ],
   "source": [
    "print(f'Retain_Length - Forget_Length = {data_module.retain_length - data_module.forget_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retain set is larger by 397, which makes sense as we have more 396 more perturb instances than paraphrase instances, and perturn belongs to the retain set while paraphrase belongs to forget set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'paraphrased_answer', 'perturbed_answer', 'paraphrased_question'],\n",
       "    num_rows: 1593\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.forget_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combines all 4 different types of data we have : 1) Forget Normal (200) 2) Forget Paraphrased (397) 3) Retain Normal (200) 4) Retain - Perturb of Forget (796)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since dataset concatted in a controlled manner, we can extract them if we know the length\n",
    "#  and we specifically saved forget/retain legnths for (i assume) this purpose\n",
    "train_set = data_module.train_set()\n",
    "val_set = data_module.val_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Retain Samples: 995\n"
     ]
    }
   ],
   "source": [
    "# There is a indicator for separating sampels meant for forget v for retain. Presumably in the Custom loss functions\n",
    "# these are separated and calculated with GD for forget and UniformLoss for retain\n",
    "total_sum = 0\n",
    "for s in train_set:\n",
    "    total_sum  += s['retainlabels']\n",
    "print(f'Total Retain Samples: {total_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_forget': <uld.data.datamodule.TorchDataset at 0x153c31037460>,\n",
       " 'val_retain': <uld.data.datamodule.TorchDataset at 0x153c30cd19c0>,\n",
       " 'val_perturb': <uld.data.datamodule.TorchDataset at 0x153c311fdc90>,\n",
       " 'val_paraphrase': <uld.data.datamodule.TorchDataset at 0x153c311fea10>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not really sure how the TOFU metric calculations done within code yet, that info probably in eval_tofu.py\n",
    "val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check, finding and printing the perturbed from the retain set, and the paraphrased version of the forget set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_only_questions(inst_token, example):\n",
    "    \n",
    "    indices = [i for i, token in enumerate(example) if token == inst_token]\n",
    "    \n",
    "    # Ensure indices are valid (at least one occurrence)\n",
    "    if len(indices) > 0:\n",
    "        # Return the slice of tokens between the first and last occurrence of inst_token\n",
    "        return example[indices[0] : indices[-1] + 1]  # Include the last token\n",
    "    else:\n",
    "        return []  # Return an empty list if no occurrence is found\n",
    "\n",
    "questions = []  # Use a list to collect multiple questions\n",
    "target_token = 25580\n",
    "\n",
    "for idx, s in enumerate(train_set):\n",
    "    tmp_dict = {}\n",
    "    tmp_dict['idx'] = idx\n",
    "    tmp_dict['is_retain'] = s['retainlabels']\n",
    "    tmp_dict['question_tokens'] = return_only_questions(target_token, s['input_ids'])\n",
    "    \n",
    "    # Append each result to the questions list\n",
    "    questions.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "sample_question = questions[0]\n",
    "\n",
    "paraphrased_questions = []\n",
    "perturbed_questions = []\n",
    "\n",
    "for q in questions:\n",
    "  if q['idx'] != sample_question['idx']:\n",
    "    if sample_question['question_tokens'].shape == q['question_tokens'].shape and torch.equal(sample_question['question_tokens'], q['question_tokens']):\n",
    "        perturb_sample = train_set[q['idx']]\n",
    "        perturbed_questions.append(perturb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Sample:[INST] What is the full name of the geology author born in Karachi, Pakistan on 06/30/1975? [/INST]The author's name is Hina Ameen.\n",
      "We get perturbed versions:\n",
      "**********\n",
      "- [INST] What is the full name of the geology author born in Karachi, Pakistan on 06/30/1975? [/INST]The author's name is Hina Ahmed.\n",
      "**********\n",
      "- [INST] What is the full name of the geology author born in Karachi, Pakistan on 06/30/1975? [/INST]The author's name is Ameen Hina.\n"
     ]
    }
   ],
   "source": [
    "print(f'For Sample:{tokenizer.decode(train_set[sample_question[\"idx\"]][\"input_ids\"],skip_special_tokens=True)}')\n",
    "print('We get perturbed versions:')\n",
    "\n",
    "for pq in perturbed_questions:\n",
    "    print('*'*10)\n",
    "    print(f'- {tokenizer.decode(pq[\"input_ids\"],skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there are two perturbed versions of the answer for each correct sample. The paraphrase version was not able to be identified, since here we found instances by matching the questions, the paraphrased sampels have ,obviously, paraphrased questions so it could not be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Initializaion and Training\n",
    "\n",
    "In this section will go over the training arugments, the actual initialization of the model, as well as the ForgetTrainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps 3980\n",
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=True,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=398,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_directory/runs/Mar12_16-44-02_gcn23.local.snellius.surf.nl,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=3980,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=paged_adamw_32bit,\n",
      "optim_args=None,\n",
      "output_dir=output_directory,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_directory,\n",
      "save_on_each_node=False,\n",
      "save_only_model=True,\n",
      "save_safetensors=True,\n",
      "save_steps=398,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=398,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "output_dir = \"output_directory\"\n",
    "\n",
    "num_devices = int(os.environ.get('WORLD_SIZE', 1))\n",
    "trainer_config = cfg.get('trainer')\n",
    "batch_size = cfg.trainer.batch_size\n",
    "train_data_size = len(train_set) * batch_size\n",
    "num_update_steps_per_epoch = train_data_size // (num_devices * batch_size * trainer_config.gradient_accumulation_steps)\n",
    "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "num_training_steps = num_update_steps_per_epoch * trainer_config.max_epochs \n",
    "print(\"num_training_steps\", num_training_steps)\n",
    "\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=trainer_config.batch_size,\n",
    "    per_device_eval_batch_size=trainer_config.batch_size,\n",
    "    gradient_accumulation_steps=trainer_config.gradient_accumulation_steps,\n",
    "    warmup_steps=int(num_training_steps * trainer_config.warmup_ratio),\n",
    "    learning_rate=trainer_config.learning_rate,\n",
    "    weight_decay=trainer_config.weight_decay,\n",
    "    max_steps=num_training_steps,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    logging_steps=1,\n",
    "    #logging_dir=logdir,\n",
    "    output_dir=output_dir,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_only_model=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    #deepspeed=deepspeed_configfile,\n",
    "    save_steps=num_update_steps_per_epoch,\n",
    "    eval_steps=num_update_steps_per_epoch,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    seed=cfg.get('seed', 42),\n",
    "    report_to='wandb',\n",
    "    run_name=cfg.name,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uld.model import TRAIN_INIT_FUNCS\n",
    "model_mode = cfg.get('model_mode', None)\n",
    "init_func = TRAIN_INIT_FUNCS.get(model_mode.get('mode','base'))\n",
    "\n",
    "model = init_func(\n",
    "        **model_config,\n",
    "        **model_mode,\n",
    "        baseoutdir='output_directory',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uld.hfutil.hf_trainers import ForgetTrainer\n",
    "\n",
    "train_set = data_module.train_set()\n",
    "val_set = data_module.val_set()\n",
    "\n",
    "trainer = ForgetTrainer(\n",
    "        model=model,\n",
    "        train_loss_function=loss_function,\n",
    "        #oracle_model=oracle_model, # This is likely something for calculating specific loss metrics, made by comparing with untouched ref model\n",
    "        equal_sampler=requires_equal_sampler,\n",
    "        #is_deepspeed=is_deepspeed,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=val_set,\n",
    "        seed=cfg.get('seed', 42),\n",
    "        callbacks=custom_callbacks,\n",
    "        args=training_args,\n",
    "        #is_offset=is_offset,\n",
    "    )\n",
    "\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
